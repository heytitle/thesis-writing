\thispagestyle{empty}
\vspace*{1.0cm}

\begin{center}
    \textbf{Abstract}
\end{center}

\vspace*{0.5cm}

\noindent

Neural networks (NNs) have been increasingly used in many applications nowadays. Despite achieving state-of-the-art performance in various domains, NNs are still considered as black boxes that one can hardly explain how they map input to output with such accurate manner.  Several techniques have been proposed recently to interpret NN predictions. Such methods include sensitivity analysis (SA), guided backprop (GB), Layer-wise Relevance Propagation (LRP), and deep Taylor decomposition (DTD).  Recurrent neural networks (RNNs) are NNs that have a recurrent mechanism. The mechanism well suits them to model sequential data.  Unlike feedforward architectures, RNNs need to learn data representations and know how to summarize and use them appropriately across time steps. Hence, well-structured RNNs that can isolate the recurrent mechanism from the representational part of the model will have an advantage of being more explainable. In this thesis, we apply the explanation techniques mentioned above to RNNs. We extensively study the impact of the RNN architecture on the quality of explanations with different RNN configurations. Our experiments are based on artificial classification problems constructed from MNIST and FashionMNIST datasets. We use cosine similarity to quantify evaluation results. Our results show that the quality of explanations from trained RNNs, achieving comparable accuracies, can be notably different. Based on our evaluations, deeper and LSTM-type RNNs have more explainable predictions regardless of the explanation methods.  Convolutional and pooling layers and stationary dropout technique are another contributors to the quality of the explanations. We also find that some explanation techniques are more sensitive to the architecture of RNNs. With a particular setting, one of the RNN architectures proposed in this thesis establishes a high level of explainability. We believe that its prediction explanations are substantially informative.

%deep Taylor decomposition(DTD) and Layer-Wise Relevance Propagation(LRP) 
%|||||||||
%
%||||||||||
%Explaining decisions of neural networks have  increaslying gained attentions from researchers. Several techniques were deveoped, however only small  
%
%Research in direction of 
%
%
%Standard (non-LSTM) recurrent neural networks have been challenging to train, but special optimization techniques such as heavy momentum makes this possible. However, the potentially strong entangling of features that results from this difficult optimization problem can cause deep Taylor or LRP-type to perform rather poorly due to their lack of global scope. LSTM networks are an alternative, but their gating function make them hard to explain by deep Taylor LRP in a fully principled manner. Ideally, the RNN should be expressible as a deep ReLU network, but also be reasonably disentangled to let deep Taylor LRP perform reasonably. The goal of this thesis will be to enrich the structure of the RNN with more layers to better isolate the recurrent mechanism from the representational part of the model. 