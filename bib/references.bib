
@article{ArrasWhatRelevantText2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.07843},
  primaryClass = {cs, stat},
  title = {"{{What}} Is {{Relevant}} in a {{Text Document}}?": {{An Interpretable Machine Learning Approach}}},
  url = {http://arxiv.org/abs/1612.07843},
  shorttitle = {"{{What}} Is {{Relevant}} in a {{Text Document}}?},
  abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
  urldate = {2017-07-19},
  date = {2016-12-22},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Learning,Statistics - Machine Learning},
  author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  file = {/Users/heytitle/Zotero/storage/7AKZETAH/Arras et al. - 2016 - What is Relevant in a Text Document An Interpr.pdf;/Users/heytitle/Zotero/storage/ZUWJYIHN/1612.html},
  year = {2016}
}

@article{MontavonExplainingnonlinearclassification2017,
  title = {Explaining Nonlinear Classification Decisions with Deep {{Taylor}} Decomposition},
  volume = {65},
  issn = {0031-3203},
  url = {http://www.sciencedirect.com/science/article/pii/S0031320316303582},
  doi = {10.1016/j.patcog.2016.11.008},
  abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  urldate = {2017-07-23},
  date = {2017-05-01},
  pages = {211--222},
  keywords = {Deep neural networks,Heatmapping,Image recognition,Relevance propagation,Taylor decomposition},
  author = {Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  file = {/Users/heytitle/Zotero/storage/ZRJN2CBI/S0031320316303582.html},
  journal = {Pattern Recognition},
  year = {May 1, 2017}
}

@article{MontavonMethodsInterpretingUnderstanding2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.07979},
  primaryClass = {cs, stat},
  title = {Methods for {{Interpreting}} and {{Understanding Deep Neural Networks}}},
  url = {http://arxiv.org/abs/1706.07979},
  abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.},
  urldate = {2017-07-24},
  date = {2017-06-24},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  file = {/Users/heytitle/Zotero/storage/DE3B5ZN9/Montavon et al. - 2017 - Methods for Interpreting and Understanding Deep Ne.pdf;/Users/heytitle/Zotero/storage/LVET2A5I/1706.html},
  year = {2017}
}

@article{KindermansPatternNetPatternLRPImproving2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.05598},
  primaryClass = {cs, stat},
  title = {{{PatternNet}} and {{PatternLRP}} -- {{Improving}} the Interpretability of Neural Networks},
  url = {http://arxiv.org/abs/1705.05598},
  abstract = {Deep learning has significantly advanced the state of the art in machine learning. However, neural networks are often considered black boxes. There is significant effort to develop techniques that explain a classifier's decisions. Although some of these approaches have resulted in compelling visualisations, there is a lack of theory of what is actually explained. Here we present an analysis of these methods and formulate a quality criterion for explanation methods. On this ground, we propose an improved method that may serve as an extension for existing back-projection and decomposition techniques.},
  urldate = {2017-07-24},
  date = {2017-05-16},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Kindermans, Pieter-Jan and Sch{\"u}tt, Kristof T. and Alber, Maximilian and M{\"u}ller, Klaus-Robert and D{\"a}hne, Sven},
  file = {/Users/heytitle/Zotero/storage/C3AJYV3A/Kindermans et al. - 2017 - PatternNet and PatternLRP -- Improving the interpr.pdf;/Users/heytitle/Zotero/storage/PQPY6K78/1705.html},
  year = {2017}
}

@article{WieschollekLearningBlindMotion2017,
  title = {Learning {{Blind Motion Deblurring}}},
  url = {http://arxiv.org/abs/1708.04208v1},
  abstract = {As handheld video cameras are now commonplace and available in every
smartphone, images and videos can be recorded almost everywhere at anytime.
However, taking a quick shot frequently yields a blurry result due to unwanted
camera shake during recording or moving objects in the scene. Removing these
artifacts from the blurry recordings is a highly ill-posed problem as neither
the sharp image nor the motion blur kernel is known. Propagating information
between multiple consecutive blurry observations can help restore the desired
sharp image or video. Solutions for blind deconvolution based on neural
networks rely on a massive amount of ground-truth data which is hard to
acquire. In this work, we propose an efficient approach to produce a
significant amount of realistic training data and introduce a novel recurrent
network architecture to deblur frames taking temporal information into account,
which can efficiently handle arbitrary spatial and temporal input sizes. We
demonstrate the versatility of our approach in a comprehensive comparison on a
number of