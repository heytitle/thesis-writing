
@article{MontavonExplainingnonlinearclassification2017,
  title = {Explaining Nonlinear Classification Decisions with Deep {{Taylor}} Decomposition},
  volume = {65},
  issn = {0031-3203},
  url = {http://www.sciencedirect.com/science/article/pii/S0031320316303582},
  doi = {10.1016/j.patcog.2016.11.008},
  abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  urldate = {2017-07-23},
  date = {2017-05-01},
  pages = {211-222},
  keywords = {Deep neural networks,Heatmapping,Image recognition,Relevance propagation,Taylor decomposition},
  author = {Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  file = {/Users/heytitle/Zotero/storage/ZRJN2CBI/S0031320316303582.html},
  journal = {Pattern Recognition},
  year = {May 1, 2017}
}

@article{KindermansPatternNetPatternLRPImproving2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.05598},
  primaryClass = {cs, stat},
  title = {{{PatternNet}} and {{PatternLRP}} -- {{Improving}} the Interpretability of Neural Networks},
  url = {http://arxiv.org/abs/1705.05598},
  abstract = {Deep learning has significantly advanced the state of the art in machine learning. However, neural networks are often considered black boxes. There is significant effort to develop techniques that explain a classifier's decisions. Although some of these approaches have resulted in compelling visualisations, there is a lack of theory of what is actually explained. Here we present an analysis of these methods and formulate a quality criterion for explanation methods. On this ground, we propose an improved method that may serve as an extension for existing back-projection and decomposition techniques.},
  urldate = {2017-07-24},
  date = {2017-05-16},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Kindermans, Pieter-Jan and Sch{\"u}tt, Kristof T. and Alber, Maximilian and M{\"u}ller, Klaus-Robert and D{\"a}hne, Sven},
  file = {/Users/heytitle/Zotero/storage/C3AJYV3A/Kindermans et al. - 2017 - PatternNet and PatternLRP -- Improving the interpr.pdf;/Users/heytitle/Zotero/storage/PQPY6K78/1705.html},
  year = {2017}
}

@article{ShahriariTakingHumanOut2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  volume = {104},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2015.2494218},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  number = {1},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  date = {2016-01},
  pages = {148-175},
  keywords = {Big data,Bayes methods,Bayesian optimization,Big Data,Big data application,decision making,Decision making,design of experiments,Design of experiments,Genomes,genomic medicine,human productivity,large-scale heterogeneous computing,Linear programming,massive complex software system,optimisation,optimization,Optimization,product quality,response surface methodology,Statistical analysis,statistical learning,storage allocation,storage architecture},
  author = {Shahriari, B. and Swersky, K. and Wang, Z. and Adams, R. P. and de Freitas, N.},
  file = {/Users/heytitle/Zotero/storage/HHWVCX3I/7352306.html},
  journal = {Proceedings of the IEEE},
  year = {January 2016}
}

@article{XiaoFashionMNISTNovelImage2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.07747},
  primaryClass = {cs, stat},
  title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
  url = {http://arxiv.org/abs/1708.07747},
  shorttitle = {Fashion-{{MNIST}}},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  urldate = {2017-12-28},
  date = {2017-08-25},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Computer Vision and Pattern Recognition},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  file = {/Users/heytitle/Zotero/storage/8Y9MAPE9/Xiao et al. - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmark.pdf;/Users/heytitle/Zotero/storage/9MPKQEQV/1708.html},
  year = {2017}
}

@article{LeCunMNISThandwrittendigit2010,
  title = {{{MNIST}} Handwritten Digit Database},
  url = {http://yann.lecun.com/exdb/mnist/},
  urldate = {2016-01-14},
  date = {2010},
  keywords = {MSc _checked character_recognition mnist network neural},
  author = {LeCun, Yann and Cortes, Corinna},
  year = {2010}
}

@inproceedings{KingmaAdamMethodStochastic2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  url = {http://arxiv.org/abs/1412.6980},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  urldate = {2017-12-29},
  date = {2014-12-22},
  keywords = {Computer Science - Learning},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  file = {/Users/heytitle/Zotero/storage/VZXMDASB/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;/Users/heytitle/Zotero/storage/TSUEKSFG/1412.html},
  journal = {Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
  year = {2014}
}

@article{AbadiTensorFlowLargeScaleMachine2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.04467},
  primaryClass = {cs},
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  url = {http://arxiv.org/abs/1603.04467},
  shorttitle = {{{TensorFlow}}},
  abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
  urldate = {2018-01-03},
  date = {2016-03-14},
  keywords = {Computer Science - Learning,Computer Science - Distributed; Parallel; and Cluster Computing},
  author = {Abadi, Mart{\'\i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  file = {/Users/heytitle/Zotero/storage/FQVBI7V4/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf;/Users/heytitle/Zotero/storage/C72L7DTQ/1603.html},
  year = {2016}
}

@article{BachPixelWiseExplanationsNonLinear2015,
  title = {On {{Pixel}}-{{Wise Explanations}} for {{Non}}-{{Linear Classifier Decisions}} by {{Layer}}-{{Wise Relevance Propagation}}},
  volume = {10},
  issn = {1932-6203},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4498753/},
  doi = {10.1371/journal.pone.0130140},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  number = {7},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  urldate = {2018-01-04},
  date = {2015-07-10},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  file = {/Users/heytitle/Zotero/storage/RU4CWF93/Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifi.pdf},
  eprinttype = {pmid},
  eprint = {26161953},
  pmcid = {PMC4498753},
  journal = {PLoS ONE},
  year = {2015}
}

@article{TalathiImprovingperformancerecurrent2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.03771},
  primaryClass = {cs},
  title = {Improving Performance of Recurrent Neural Network with Relu Nonlinearity},
  url = {http://arxiv.org/abs/1511.03771},
  abstract = {In recent years significant progress has been made in successfully training recurrent neural networks (RNNs) on sequence learning problems involving long range temporal dependencies. The progress has been made on three fronts: (a) Algorithmic improvements involving sophisticated optimization techniques, (b) network design involving complex hidden layer nodes and specialized recurrent layer connections and (c) weight initialization methods. In this paper, we focus on recently proposed weight initialization with identity matrix for the recurrent weights in a RNN. This initialization is specifically proposed for hidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple dynamical systems perspective on weight initialization process, which allows us to propose a modified weight initialization strategy. We show that this initialization technique leads to successfully training RNNs composed of ReLUs. We demonstrate that our proposal produces comparable or better solution for three toy problems involving long range temporal structure: the addition problem, the multiplication problem and the MNIST classification problem using sequence of pixels. In addition, we present results for a benchmark action recognition problem.},
  urldate = {2018-01-06},
  date = {2015-11-11},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Talathi, Sachin S. and Vartak, Aniket},
  file = {/Users/heytitle/Zotero/storage/UWMP5FI5/Talathi and Vartak - 2015 - Improving performance of recurrent neural network .pdf;/Users/heytitle/Zotero/storage/G2ZVPXJ9/1511.html},
  year = {2015}
}

@article{GuntukuUnderstandingDeepRepresentations2016,
  title = {Understanding {{Deep Representations Learned}} in {{Modeling Users Likes}}},
  volume = {25},
  issn = {1057-7149},
  doi = {10.1109/TIP.2016.2576278},
  abstract = {Automatically understanding and discriminating different users' liking for an image is a challenging problem. This is because the relationship between image features (even semantic ones extracted by existing tools, viz., faces, objects, and so on) and users' likes is non-linear, influenced by several subtle factors. This paper presents a deep bi-modal knowledge representation of images based on their visual content and associated tags (text). A mapping step between the different levels of visual and textual representations allows for the transfer of semantic knowledge between the two modalities. Feature selection is applied before learning deep representation to identify the important features for a user to like an image. The proposed representation is shown to be effective in discriminating users based on images they like and also in recommending images that a given user likes, outperforming the state-of-the-art feature representations by 15 \%-20\%. Beyond this test-set performance, an attempt is made to qualitatively understand the representations learned by the deep architecture used to model user likes.},
  number = {8},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. Image Process.},
  date = {2016-08},
  pages = {3762-3774},
  keywords = {learning (artificial intelligence),automatic user liking discriminating,automatic user liking understanding,Clutter,Context,Context modeling,deep bimodal image knowledge representation,deep representation learning,Deep representations,Deep Representations,feature extraction,Feature extraction,feature representations,feature selection,image features,image recommendation,Image Recommendation,image representation,image retrieval,knowledge representation,semantic knowledge transfer,semantic structures,Semantic Structures,Semantics,tags,textual representations,user likes,User Likes,user liking modeling,visual content,visual representations,Visualization},
  author = {Guntuku, S. C. and Zhou, J. T. and Roy, S. and Lin, W. and Tsang, I. W.},
  file = {/Users/heytitle/Zotero/storage/R427XJJ2/7484658.html},
  journal = {IEEE Transactions on Image Processing},
  year = {August 2016}
}

@report{ErhanUnderstandingRepresentationsLearned2010,
  title = {Understanding {{Representations Learned}} in {{Deep Architectures}}},
  abstract = {Deep architectures have demonstrated state-of-the-art performance in a variety of settings, especially with vision datasets. Deep learning algorithms are based on learning several levels of representation of the input. Beyond test-set performance, there is a need for qualitative comparisons of the solutions learned by various deep architectures, focused on those learned representations. One of the goals of our research is to improve tools for finding good qualitative interpretations of high level features learned by such models. We also seek to gain insight into the invariances learned by deep networks. To this end, we contrast and compare several techniques for finding such interpretations. We applied our techniques on Stacked Denoising Auto-Encoders and Deep Belief Networks, trained on several vision datasets. We show that consistent filter-like interpretation is possible and simple to accomplish at the unit level. The tools developed make it possible to analyze deep models in more depth and accomplish the tracing of invariance manifolds for each of the hidden units. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work.},
  number = {1355},
  institution = {{Universit{\'e} de Montr{\'e}al/DIRO}},
  date = {2010-10},
  author = {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua},
  year = {October 2010}
}

@inproceedings{BinderLayerWiseRelevancePropagation2016,
  langid = {english},
  title = {Layer-{{Wise Relevance Propagation}} for {{Neural Networks}} with {{Local Renormalization Layers}}},
  isbn = {978-3-319-44780-3 978-3-319-44781-0},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-44781-0_8},
  doi = {10.1007/978-3-319-44781-0_8},
  abstract = {Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets.},
  eventtitle = {International {{Conference}} on {{Artificial Neural Networks}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2016},
  series = {Lecture Notes in Computer Science},
  publisher = {{Springer, Cham}},
  urldate = {2018-01-07},
  date = {2016},
  pages = {63-71},
  author = {Binder, Alexander and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  file = {/Users/heytitle/Zotero/storage/3ALFZF4Z/978-3-319-44781-0_8.html},
  journal = {Artificial Neural Networks and Machine Learning – ICANN 2016},
  year = {2016}
}

@article{SimonyanDeepConvolutionalNetworks2013,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  volume = {abs/1312.6034},
  url = {http://arxiv.org/abs/1312.6034},
  journaltitle = {CoRR},
  date = {2013},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal = {CoRR},
  year = {2013}
}

@inproceedings{BachAnalyzingclassifiersFisher2016,
  title = {Analyzing Classifiers: {{Fisher}} Vectors and Deep Neural Networks},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  date = {2016},
  pages = {2912-2920},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Muller, Klaus-Robert and Samek, Wojciech},
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2016}
}

@article{TielemanLectureRmsPropDivide2012,
  title = {Lecture 6.5---{{RmsProp}}: {{Divide}} the Gradient by a Running Average of Its Recent Magnitude},
  date = {2012},
  author = {Tieleman, T. and Hinton, G.},
  howpublished = {COURSERA: Neural Networks for Machine Learning},
  year = {2012}
}

@article{SimonyanVeryDeepConvolutional2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1556},
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  volume = {abs/1409.1556},
  url = {http://arxiv.org/abs/1409.1556},
  journaltitle = {CoRR},
  date = {2014},
  author = {Simonyan, Karen and Zisserman, Andrew},
  biburl = {https://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  journal = {CoRR},
  year = {2014}
}

@inproceedings{LeCunGradientBasedLearningApplied2001,
  title = {Gradient-{{Based Learning Applied}} to {{Document Recognition}}},
  booktitle = {Intelligent {{Signal Processing}}},
  publisher = {{IEEE Press}},
  date = {2001},
  pages = {306-351},
  author = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  original = {orig/lecun-01a.ps.gz},
  editors = {Haykin, S. and Kosko, B.},
  journal = {Intelligent Signal Processing},
  year = {2001}
}

@incollection{KrizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  publisher = {{Curran Associates, Inc.}},
  date = {2012},
  pages = {1097-1105},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  journal = {Advances in Neural Information Processing Systems 25},
  year = {2012}
}

@inproceedings{ChoLearningPhraseRepresentations2014a,
  location = {{Doha, Qatar}},
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}--{{Decoder}} for {{Statistical Machine Translation}}},
  url = {http://www.aclweb.org/anthology/D14-1179},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  publisher = {{Association for Computational Linguistics}},
  date = {2014-10},
  pages = {1724-1734},
  author = {Cho, Kyunghyun and van Merri{\"e}nboer, Bart and G{\"u}l{\c c}ehre, {\c C}a{\u g}lar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  options = {useprefix=true},
  journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2014}
}

@inproceedings{LeeConvolutionalDeepBelief2009,
  location = {{Montreal, Quebec, Canada}},
  title = {Convolutional {{Deep Belief Networks}} for {{Scalable Unsupervised Learning}} of {{Hierarchical Representations}}},
  isbn = {978-1-60558-516-1},
  url = {http://doi.acm.org/10.1145/1553374.1553453},
  doi = {10.1145/1553374.1553453},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}}},
  series = {ICML '09},
  publisher = {{ACM}},
  date = {2009},
  pages = {609-616},
  author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
  numpages = {8},
  acmid = {1553453},
  journal = {Proceedings of the 26th Annual International Conference on Machine Learning},
  year = {2009}
}

@inproceedings{NguyenSynthesizingpreferredinputs2016a,
  title = {Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks},
  url = {http://lmb.informatik.uni-freiburg.de/Publications/2016/DB16d},
  booktitle = {Advances in {{Neural Information Processing Systems}} ({{NIPS}})},
  date = {2016},
  author = {Nguyen, A. and Dosovitskiy, A. and Yosinski, J. and Brox, T. and Clune, J.},
  journal = {Advances in Neural Information Processing Systems (NIPS)},
  year = {2016}
}

@online{OlahUnderstandingLSTMNetworks2015,
  title = {Understanding {{LSTM Networks}}},
  url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  urldate = {2018-04-01},
  date = {2015},
  author = {Olah, Christopher},
  year = {2015}
}

@inproceedings{MelisStateArtEvaluation2018,
  title = {On the {{State}} of the {{Art}} of {{Evaluation}} in {{Neural Language Models}}},
  url = {https://openreview.net/forum?id=ByJHuTgA-},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  date = {2018},
  author = {Melis, G{\'a}bor and Dyer, Chris and Blunsom, Phil},
  journal = {International Conference on Learning Representations},
  year = {2018}
}

@online{LeonMakingSimpleNeural2017,
  title = {Making a {{Simple Neural Network}}},
  url = {https://becominghuman.ai/making-a-simple-neural-network-2ea1de81ec20},
  abstract = {What are we making ? We'll try making a simple \& minimal Neural Network which we will explain and train to identify something, there will\ldots{}},
  journaltitle = {Becoming Human: Artificial Intelligence Magazine},
  urldate = {2018-04-01},
  date = {2017-04-09T19:53:29.184Z},
  author = {Leon, Keno},
  file = {/Users/heytitle/Zotero/storage/IJDZZD2N/making-a-simple-neural-network-2ea1de81ec20.html},
  journal = {Becoming Human: Artificial Intelligence Magazine},
  year = {2017}
}

@article{OlahBuildingBlocksInterpretability2018,
  title = {The {{Building Blocks}} of {{Interpretability}}},
  doi = {10.23915/distill.00010},
  journaltitle = {Distill},
  date = {2018},
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  note = {https://distill.pub/2018/building-blocks},
  journal = {Distill},
  year = {2018}
}

@article{SmilkovSmoothGradremovingnoise2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03825},
  title = {{{SmoothGrad}}: Removing Noise by Adding Noise},
  volume = {abs/1706.03825},
  url = {http://arxiv.org/abs/1706.03825},
  journaltitle = {CoRR},
  date = {2017},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda B. and Wattenberg, Martin},
  biburl = {https://dblp.org/rec/bib/journals/corr/SmilkovTKVW17},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  journal = {CoRR},
  year = {2017}
}

@article{MontavonMethodsinterpretingunderstanding2018,
  title = {Methods for Interpreting and Understanding Deep Neural Networks},
  volume = {73},
  issn = {1051-2004},
  url = {http://www.sciencedirect.com/science/article/pii/S1051200417302385},
  doi = {https://doi.org/10.1016/j.dsp.2017.10.011},
  journaltitle = {Digital Signal Processing},
  shortjournal = {Digit. Signal Process.},
  date = {2018},
  pages = {1-15},
  keywords = {Deep neural networks,Taylor decomposition,Activation maximization,Layer-wise relevance propagation,Sensitivity analysis},
  author = {Montavon, Gr{\'e}goire and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  journal = {Digital Signal Processing},
  year = {2018}
}

@inproceedings{ZeilerVisualizingUnderstandingConvolutional2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  url = {https://doi.org/10.1007/978-3-319-10590-1_53},
  doi = {10.1007/978-3-319-10590-1_53},
  booktitle = {Computer {{Vision}} - {{ECCV}} 2014 - 13th {{European Conference}}, {{Zurich}}, {{Switzerland}}, {{September}} 6-12, 2014, {{Proceedings}}, {{Part I}}},
  date = {2014},
  pages = {818-833},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  crossref = {DBLP:conf/eccv/2014-1},
  biburl = {https://dblp.org/rec/bib/conf/eccv/ZeilerF14},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  journal = {Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I},
  year = {2014}
}

@inproceedings{JozefowiczEmpiricalExplorationRecurrent2015,
  title = {An {{Empirical Exploration}} of {{Recurrent Network Architectures}}},
  url = {http://jmlr.org/proceedings/papers/v37/jozefowicz15.html},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}, {{ICML}} 2015, {{Lille}}, {{France}}, 6-11 {{July}} 2015},
  date = {2015},
  pages = {2342-2350},
  author = {J{\'o}zefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  crossref = {DBLP:conf/icml/2015},
  biburl = {https://dblp.org/rec/bib/conf/icml/JozefowiczZS15},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  journal = {Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015},
  year = {2015}
}

@inproceedings{GalTheoreticallyGroundedApplication2016,
  title = {A {{Theoretically Grounded Application}} of {{Dropout}} in {{Recurrent Neural Networks}}},
  url = {http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29: {{Annual Conference}} on {{Neural Information Processing Systems}} 2016, {{December}} 5-10, 2016, {{Barcelona}}, {{Spain}}},
  date = {2016},
  pages = {1019-1027},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  crossref = {DBLP:conf/nips/2016},
  biburl = {https://dblp.org/rec/bib/conf/nips/GalG16},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  journal = {Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain},
  year = {2016}
}

@article{ZeilerADADELTAAdaptiveLearning2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1212.5701},
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  volume = {abs/1212.5701},
  url = {http://arxiv.org/abs/1212.5701},
  journaltitle = {CoRR},
  date = {2012},
  author = {Zeiler, Matthew D.},
  biburl = {https://dblp.org/rec/bib/journals/corr/abs-1212-5701},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  journal = {CoRR},
  year = {2012}
}

@inproceedings{PhanAudioSceneClassification2017,
  title = {Audio {{Scene Classification}} with {{Deep Recurrent Neural Networks}}},
  url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0101.html},
  booktitle = {Interspeech 2017, 18th {{Annual Conference}} of the {{International Speech Communication Association}}, {{Stockholm}}, {{Sweden}}, {{August}} 20-24, 2017},
  date = {2017},
  pages = {3043-3047},
  author = {Phan, Huy and Koch, Philipp and Katzberg, Fabrice and Maa{\ss}, Marco and Mazur, Radoslaw and Mertins, Alfred},
  crossref = {DBLP:conf/interspeech/2017},
  biburl = {https://dblp.org/rec/bib/conf/interspeech/PhanKKMMM17},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  journal = {Interspeech 2017, 18th Annual Conference of the International Speech Communication Association, Stockholm, Sweden, August 20-24, 2017},
  year = {2017}
}

@inproceedings{SundararajanAxiomaticAttributionDeep2017a,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  volume = {70},
  url = {http://proceedings.mlr.press/v70/sundararajan17a.html},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2017, {{Sydney}}, {{NSW}}, {{Australia}}, 6-11 {{August}} 2017},
  series = {Proceedings of Machine Learning Research},
  publisher = {{PMLR}},
  urldate = {2018-04-13},
  date = {2017},
  pages = {3319--3328},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  editor = {Precup, Doina and Teh, Yee Whye},
  journal = {Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017},
  year = {2017}
}

@inproceedings{JiaCaffeConvolutionalArchitecture2014,
  title = {Caffe: {{Convolutional Architecture}} for {{Fast Feature Embedding}}},
  isbn = {978-1-4503-3063-3},
  url = {http://doi.acm.org/10.1145/2647868.2654889},
  doi = {10.1145/2647868.2654889},
  shorttitle = {Caffe},
  booktitle = {Proceedings of the {{ACM International Conference}} on {{Multimedia}}, {{MM}} '14, {{Orlando}}, {{FL}}, {{USA}}, {{November}} 03 - 07, 2014},
  publisher = {{ACM}},
  urldate = {2018-04-13},
  date = {2014},
  pages = {675--678},
  author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross B. and Guadarrama, Sergio and Darrell, Trevor},
  editor = {Hua, Kien A. and Rui, Yong and Steinmetz, Ralf and Hanjalic, Alan and Natsev, Apostol and Zhu, Wenwu},
  journal = {Proceedings of the ACM International Conference on Multimedia, MM '14, Orlando, FL, USA, November 03 - 07, 2014},
  year = {2014}
}

@inproceedings{HeDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  isbn = {978-1-4673-8851-1},
  doi = {10.1109/CVPR.2016.90},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}, {{CVPR}} 2016, {{Las Vegas}}, {{NV}}, {{USA}}, {{June}} 27-30, 2016},
  publisher = {{IEEE Computer Society}},
  date = {2016},
  pages = {770--778},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016},
  year = {2016}
}

@article{SrivastavaDropoutsimpleway2014,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  volume = {15},
  url = {http://dl.acm.org/citation.cfm?id=2670313},
  shorttitle = {Dropout},
  number = {1},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  urldate = {2018-04-13},
  date = {2014},
  pages = {1929--1958},
  author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal = {Journal of Machine Learning Research},
  year = {2014}
}

@inproceedings{ArrasExplainingRecurrentNeural2017,
  title = {Explaining {{Recurrent Neural Network Predictions}} in {{Sentiment Analysis}}},
  isbn = {978-1-945626-95-1},
  url = {https://aclanthology.info/papers/W17-5221/w17-5221},
  booktitle = {Proceedings of the 8th {{Workshop}} on {{Computational Approaches}} to {{Subjectivity}}, {{Sentiment}} and {{Social Media Analysis}}, {{WASSA}}@{{EMNLP}} 2017, {{Copenhagen}}, {{Denmark}}, {{September}} 8, 2017},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2018-04-13},
  date = {2017},
  pages = {159--168},
  author = {Arras, Leila and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  editor = {Balahur, Alexandra and Mohammad, Saif M. and van der Goot, Erik},
  journal = {Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, WASSA@EMNLP 2017, Copenhagen, Denmark, September 8, 2017},
  year = {2017}
}

@inproceedings{SzegedyGoingdeeperconvolutions2015,
  title = {Going Deeper with Convolutions},
  isbn = {978-1-4673-6964-0},
  doi = {10.1109/CVPR.2015.7298594},
  booktitle = {{{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}, {{CVPR}} 2015, {{Boston}}, {{MA}}, {{USA}}, {{June}} 7-12, 2015},
  publisher = {{IEEE Computer Society}},
  date = {2015},
  pages = {1--9},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott E. and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  journal = {IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015},
  year = {2015}
}

@inproceedings{WieschollekLearningBlindMotion2017,
  title = {Learning {{Blind Motion Deblurring}}},
  isbn = {978-1-5386-1032-9},
  doi = {10.1109/ICCV.2017.34},
  booktitle = {{{IEEE International Conference}} on {{Computer Vision}}, {{ICCV}} 2017, {{Venice}}, {{Italy}}, {{October}} 22-29, 2017},
  publisher = {{IEEE Computer Society}},
  date = {2017},
  pages = {231--240},
  author = {Wieschollek, Patrick and Hirsch, Michael and Sch{\"o}lkopf, Bernhard and Lensch, Hendrik P. A.},
  journal = {IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017},
  year = {2017}
}

@article{HochreiterLongShortTermMemory1997,
  title = {Long {{Short}}-{{Term Memory}}},
  volume = {9},
  doi = {10.1162/neco.1997.9.8.1735},
  number = {8},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comput.},
  date = {1997},
  pages = {1735--1780},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal = {Neural Computation},
  year = {1997}
}

@article{GreffLSTMSearchSpace2017,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  volume = {28},
  doi = {10.1109/TNNLS.2016.2582924},
  shorttitle = {{{LSTM}}},
  number = {10},
  journaltitle = {IEEE Trans. Neural Netw. Learning Syst.},
  shortjournal = {IEEE Trans Neural Netw Learn. Syst},
  date = {2017},
  pages = {2222--2232},
  author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutn{\'\i}k, Jan and Steunebrink, Bas R. and Schmidhuber, J{\"u}rgen},
  journal = {IEEE Trans. Neural Netw. Learning Syst.},
  year = {2017}
}

@inproceedings{SpringenbergStrivingSimplicityAll2015a,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  url = {http://lmb.informatik.uni-freiburg.de/Publications/2015/DB15a},
  booktitle = {{{ICLR}} (Workshop Track)},
  date = {2015},
  author = {Springenberg, J.T. and Dosovitskiy, A. and Brox, T. and Riedmiller, M.},
  journal = {ICLR (workshop track)},
  year = {2015}
}

@report{ErhanVisualizingHigherLayerFeatures2009,
  title = {Visualizing {{Higher}}-{{Layer Features}} of a {{Deep Network}}},
  abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work},
  number = {1341},
  institution = {{University of Montreal}},
  date = {2009-06},
  author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  note = {Also presented at the ICML 2009 Workshop on Learning Feature Hierarchies, Montr{\'e}al, Canada.},
  year = {2009}
}

@article{ArrasWhatrelevanttext2017,
  title = {"{{What}} Is Relevant in a Text Document?": {{An}} Interpretable Machine Learning Approach},
  volume = {12},
  url = {https://doi.org/10.1371/journal.pone.0181142},
  doi = {10.1371/journal.pone.0181142},
  abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text's category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
  number = {8},
  journaltitle = {PLOS ONE},
  date = {2017-08},
  pages = {1-23},
  author = {Arras, Leila and Horn, Franziska and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  publisher = {{Public Library of Science}},
  journal = {PLOS ONE},
  year = {2017}
}

@inproceedings{RibeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  isbn = {978-1-4503-4232-2},
  url = {http://doi.acm.org/10.1145/2939672.2939778},
  doi = {10.1145/2939672.2939778},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}, {{San Francisco}}, {{CA}}, {{USA}}, {{August}} 13-17, 2016},
  publisher = {{ACM}},
  urldate = {2018-04-13},
  date = {2016},
  pages = {1135--1144},
  author = {Ribeiro, Marco T{\'u}lio and Singh, Sameer and Guestrin, Carlos},
  editor = {Krishnapuram, Balaji and Shah, Mohak and Smola, Alexander J. and Aggarwal, Charu C. and Shen, Dou and Rastogi, Rajeev},
  journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016},
  year = {2016}
}

@inproceedings{ZagoruykoWideResidualNetworks2016,
  title = {Wide {{Residual Networks}}},
  url = {http://www.bmva.org/bmvc/2016/papers/paper087/index.html},
  booktitle = {Proceedings of the {{British Machine Vision Conference}} 2016, {{BMVC}} 2016, {{York}}, {{UK}}, {{September}} 19-22, 2016},
  publisher = {{BMVA Press}},
  urldate = {2018-04-13},
  date = {2016},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  editor = {Wilson, Richard C. and Hancock, Edwin R. and Smith, William A. P.},
  journal = {Proceedings of the British Machine Vision Conference 2016, BMVC 2016, York, UK, September 19-22, 2016},
  year = {2016}
}

@inproceedings{ZuradaSensitivityAnalysisMinimization1994,
  title = {Sensitivity {{Analysis}} for {{Minimization}} of {{Input Data Dimension}} for {{Feedforward Neural Network}}},
  isbn = {978-0-7803-1916-5},
  doi = {10.1109/ISCAS.1994.409622},
  booktitle = {1994 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}}, {{ISCAS}} 1994, {{London}}, {{England}}, {{UK}}, {{May}} 30 - {{June}} 2, 1994},
  publisher = {{IEEE}},
  date = {1994},
  pages = {447--450},
  author = {Zurada, Jacek M. and Malinowski, Aleksander and Cloete, Ian},
  journal = {1994 IEEE International Symposium on Circuits and Systems, ISCAS 1994, London, England, UK, May 30 - June 2, 1994},
  year = {1994}
}

@article{KhanClassificationdiagnosticprediction2001,
  title = {Classification and Diagnostic Prediction of Cancers Using Gene Expression Profiling and Artificial Neural Networks},
  volume = {7},
  issn = {1078-8956},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1282521/},
  doi = {10.1038/89044},
  abstract = {The purpose of this study was to develop a method of classifying cancers to specific diagnostic categories based on their gene expression signatures using artificial neural networks (ANNs). We trained the ANNs using the small, round blue-cell tumors (SRBCTs) as a model. These cancers belong to four distinct diagnostic categories and often present diagnostic dilemmas in clinical practice. The ANNs correctly classified all samples and identified the genes most relevant to the classification. Expression of several of these genes has been reported in SRBCTs, but most have not been associated with these cancers. To test the ability of the trained ANN models to recognize SRBCTs, we analyzed additional blinded samples that were not previously used for the training procedure, and correctly classified them in all cases. This study demonstrates the potential applications of these methods for tumor diagnosis and the identification of candidate targets for therapy.},
  number = {6},
  journaltitle = {Nature medicine},
  shortjournal = {Nat Med},
  urldate = {2018-04-14},
  date = {2001-06},
  pages = {673-679},
  author = {Khan, Javed and Wei, Jun S. and Ringn{\'e}r, Markus and Saal, Lao H. and Ladanyi, Marc and Westermann, Frank and Berthold, Frank and Schwab, Manfred and Antonescu, Cristina R. and Peterson, Carsten and Meltzer, Paul S.},
  file = {/Users/heytitle/Zotero/storage/76BWE74Q/Khan et al. - 2001 - Classification and diagnostic prediction of cancer.pdf},
  eprinttype = {pmid},
  eprint = {11385503},
  pmcid = {PMC1282521},
  journal = {Nature medicine},
  year = {2001}
}

@inproceedings{LandeckerInterpretingindividualclassifications2013,
  title = {Interpreting Individual Classifications of Hierarchical Networks},
  isbn = {978-1-4673-5895-8},
  doi = {10.1109/CIDM.2013.6597214},
  booktitle = {{{IEEE Symposium}} on {{Computational Intelligence}} and {{Data Mining}}, {{CIDM}} 2013, {{Singapore}}, 16-19 {{April}}, 2013},
  publisher = {{IEEE}},
  date = {2013},
  pages = {32--38},
  author = {Landecker, Will and Thomure, Michael D. and Bettencourt, Lu{\'\i}s M. A. and Mitchell, Melanie and Kenyon, Garrett T. and Brumby, Steven P.},
  journal = {IEEE Symposium on Computational Intelligence and Data Mining, CIDM 2013, Singapore, 16-19 April, 2013},
  year = {2013}
}

@inproceedings{PoulinVisualExplanationEvidence2006,
  title = {Visual {{Explanation}} of {{Evidence}} with {{Additive Classifiers}}},
  url = {http://www.aaai.org/Library/AAAI/2006/aaai06-301.php},
  booktitle = {Proceedings, {{The Twenty}}-{{First National Conference}} on {{Artificial Intelligence}} and the {{Eighteenth Innovative Applications}} of {{Artificial Intelligence Conference}}, {{July}} 16-20, 2006, {{Boston}}, {{Massachusetts}}, {{USA}}},
  publisher = {{AAAI Press}},
  urldate = {2018-04-14},
  date = {2006},
  pages = {1822--1829},
  author = {Poulin, Brett and Eisner, Roman and Szafron, Duane and Lu, Paul and Greiner, Russell and Wishart, David S. and Fyshe, Alona and Pearcy, Brandon and Macdonell, Cam and Anvik, John},
  journal = {Proceedings, The Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference, July 16-20, 2006, Boston, Massachusetts, USA},
  year = {2006}
}

@unpublished{BazenTaylorDecompositionUnified2013,
  title = {The {{Taylor Decomposition}}: {{A Unified Generalization}} of the {{Oaxaca Method}} to {{Nonlinear Models}}},
  url = {https://halshs.archives-ouvertes.fr/halshs-00828790},
  date = {2013-05},
  keywords = {nonlinear models,Oaxaca decomposition},
  author = {Bazen, Stephen and Joutard, Xavier},
  hal_id = {halshs-00828790},
  hal_version = {v1},
  note = {Ce Working Paper fait l'objet d'une publication inJournal of Economic and Social Measurement, IOS Press, 2017, 42 (2), pp.101 - 121. https://content.iospress.com/articles/journal-of-economic-and-social-measurement/jem439. 10.3233/JEM-170439. hal-01684635},
  year = {2013}
}

@inproceedings{GlorotUnderstandingdifficultytraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  volume = {9},
  url = {http://www.jmlr.org/proceedings/papers/v9/glorot10a.html},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}, {{AISTATS}} 2010, {{Chia Laguna Resort}}, {{Sardinia}}, {{Italy}}, {{May}} 13-15, 2010},
  series = {JMLR Proceedings},
  publisher = {{JMLR.org}},
  urldate = {2018-04-16},
  date = {2010},
  pages = {249--256},
  author = {Glorot, Xavier and Bengio, Yoshua},
  editor = {Teh, Yee Whye and Titterington, D. Mike},
  journal = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010},
  year = {2010}
}

@article{KarpathyVisualizingUnderstandingRecurrent2015,
  title = {Visualizing and {{Understanding Recurrent Networks}}},
  volume = {abs/1506.02078},
  url = {http://arxiv.org/abs/1506.02078},
  journaltitle = {CoRR},
  urldate = {2018-04-21},
  date = {2015},
  author = {Karpathy, Andrej and Johnson, Justin and Li, Fei-Fei},
  journal = {CoRR},
  year = {2015}
}

@inproceedings{Pascanudifficultytrainingrecurrent2013,
  location = {{Atlanta, Georgia, USA}},
  title = {On the Difficulty of Training Recurrent Neural Networks},
  volume = {28},
  url = {http://proceedings.mlr.press/v28/pascanu13.html},
  abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  series = {Proceedings of Machine Learning Research},
  publisher = {{PMLR}},
  date = {2013},
  pages = {1310-1318},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  editor = {Dasgupta, Sanjoy and McAllester, David},
  number = {3},
  journal = {Proceedings of the 30th International Conference on Machine Learning},
  year = {2013}
}

@article{BengioLearninglongtermdependencies1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  volume = {5},
  issn = {1045-9227},
  doi = {10.1109/72.279181},
  number = {2},
  journaltitle = {IEEE Transactions on Neural Networks},
  shortjournal = {IEEE Trans. Neural Netw.},
  date = {1994-03},
  pages = {157-166},
  keywords = {Neural networks,learning (artificial intelligence),Computer networks,Cost function,Delay effects,Discrete transforms,Displays,efficient learning,gradient descent,input/output sequence mapping,Intelligent networks,long-term dependencies,Neurofeedback,numerical analysis,prediction problems,Production,production problems,recognition,recurrent neural nets,recurrent neural network training,Recurrent neural networks,temporal contingencies},
  author = {Bengio, Y. and Simard, P. and Frasconi, P.},
  journal = {IEEE Transactions on Neural Networks},
  year = {1994}
}


