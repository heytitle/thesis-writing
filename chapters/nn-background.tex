\chapter{Background}\label{cha:chapter3}

\section{Neural Networks}
Neural networks(NN) are a type of machine learning algorithms that are inspired by how human brain works.  In particular, NN have units called neurons connecting together similar to the way neurons in our brain do. These connections allow NN to build hierarchical representations that are necessary to perform an objective task. \addfigure{\ref{fig:nn_simple}} illustrates a reaction task that neurons in our brain perform together to achieve.

 \begin{figure}[ht!]
    \begin{center}

\includegraphics[width=\textwidth]{nn_simple}
\caption{An illustration of how neurons in human brain cooperate together to sense the pain and react accordingly.}
\small{Source : \cite{LeonMakingSimpleNeural2017}}
\label{fig:nn_simple}

\end{center}
\end{figure}

 \begin{figure}[ht!]
    \begin{center}

\includegraphics[width=0.8\textwidth]{sketch/typical_nn_structure}
\caption[]{A general structure of neural networks and details of a neuron's connectivity and activity.}
\label{fig:nn_typical_structure}

\end{center}
\end{figure}


% \begin{figure}[ht!]
%	\begin{center}
%
%		\includegraphics[width=0.5\textwidth]{sketch/a_neuron}
%		\caption{Connectivity and activity of a neuron}
%		\label{fig:a_neuron}
%	\end{center}
%\end{figure}


\addfigure{\ref{fig:nn_typical_structure}} illustrates a general structure of NN. The network has input layer, output layer and hidden layers, which can be analogously viewed to sensory, motor and inter neurons in \addfigure{\ref{fig:nn_simple}}. The figure also shows connections of a neuron to other neurons in previous and following layer. Given an objective task, the goal is to construct connections between these neurons such that the network can transform an input sample into a desired output.  This connections are determined by trainable variables, called weights and biases. They are denoted by $w_{i\rightarrow j}, w_{j\rightarrow k}$ and $b_j$ respectively in the figure. These variables will be learned  during \textit{training} process.  In this example, the objective task is to classify what is the given digit.


Consider a given set of $p$ training samples $\mathcal{D} = \{ \xa, \ya) \}_{\alpha=1}^{p}$,  there are 3 primary components to build a NN, namely  
\begin{enumerate}
	\item \textbf{Network architecture} defines the configuration of NN. Some of important settings are number of layers and number of neuron in each layer and type of activation function. These settings determine neurons' activity and how they communicate to each other through connections governed by trainable weights and biases. Typically, the weights and biases are denoted as $\patvector{\theta}$. Mathematically, NN can be viewed as a function $f$ with parameters $\patvector{\theta}$ that nonlinearly transforms an input $\xa\in \mathbb{R}^d $ to some output $f(\xa)$.
	\item \textbf{Loss function $L$}  is a measurement corresponding to the objective task. It quantifies how far NN output $f(\xa)$ is the true output $\ya$. Loss averaged over training samples is a major contributor to\textit{Cost} function $J$, a function that describes the objective of learning. Regularization is another term in $J$. \todo{wrtiing Regularizationl}.
	\item \textbf{Learning algorithm} is responsible for optimizing trainable variables in the network such that the the cost function is minimized. Practically, we learn this variables through optimizing the cost of training samples \textit{Empirical Error}. This is a proxy to optimize the cost of the data distribution(\textit{Generalization Error}). 
\end{enumerate}

Hence, the goal of this empirical training process can be summarized as follows : 

\begin{align} \label{eq:nn_opt}
	\patvector{\hat{\theta}} = \patarg{min}{\theta} \underbrace{\frac{1}{p}  \sum_{\alpha=1}^p L( f(\xa), \ya) }_{J}
\end{align}

\subsection{Loss functions}
Choosing loss function is depend on the objective that the network is being trained to solve. For classification problems, such as digit classification, whose goal is to categorize $\x$ into a category $C$ from $K$ categories, $f : \x \in \mathbb{R}^d  \mapsto C \in \{ C_k \}_{k=1}^K$, \textit{cross entropy} is the loss function for this purpose.
$$
L_{\text{CE}} = - \sum_{i} y_k \log \hat{y}_k,
$$
where $y_i \in [0, 1]$ and $\hat{y}_i \in [0, 1]$ are true and predicted probability that $\x$ belongs to $C_k$ respectively. Denote $\patvector{z} = f(\x) \in \mathbb{R}^{K}$. $\hat{y}_k$ is computed via \textit{softmax} function :
$$
\hat{y}_k = \frac{e^{z_k}}{ \sum_{k=1}^K{e^{z_k}} }
$$ 

For regression problems, $ f : \x \in \mathbb{R}^d  \mapsto \mathbb{R}$, such as price forecast,\textit{Mean Squared Error}(MSE) is the loss function.
$$
L_{\text{MSE}} = (f(\x) - y)^2
$$

This is a brief introduction to loss functions widely used in machine learning. More loss functions do exist and are beyond scope of the thesis to cover.

\subsection{Learning Algorithm : Gradient Descent and Backpropagation}
Due to substantial number of trainable variables in a neural network, it is crucial to solve the optimization (\ref{eq:nn_opt}) efficiently. The answer to  this high dimensional problem is to use a repeated procedure, called \textit{Gradient Descent}.  \addfigure{\ref{fig:gradent_descent_toy}} provides an intuition of the method. Consider a  function $J(\theta)$ on the figure as a toy example of a cost function of a NN with parameter $\theta$. The figure shows that if we gradually adjust $\theta$ in the opposite direction of gradient, $	-\frac{d L(\theta)}{d \theta}$,  with a proper step size $\lambda$ (\textit{learning rate}), we will eventually reach one of the minimals. This adjustment  is formally summarized in (\ref{eq:gradient_update}).


% In this case, $\hat{\theta}$ can trivially computed by solving
%
%\begin{align}
%	\frac{d L(\theta)}{d \theta}  \stackrel{!}{=} 0
%	\label{eq:simple_solve_for_thetha}
%\end{align}

\begin{figure}[!hbt]
    \begin{center}
		\includegraphics[width=0.6\textwidth]{sketch/gradient_intuition}
		\caption[]{An illustration of Gradient Descent}
		\label{fig:gradent_descent_toy}
	\end{center}
\end{figure}

\begin{align}
 \theta_i \leftarrow \theta_i - \lambda  \frac{\partial J }{\partial \theta_i}, \forall \theta_i \in \btheta
\label{eq:gradient_update}
\end{align}


Let's consider again the NN shown in \addfigure{\ref{fig:nn_typical_structure}}. Assume that the network uses an activation function $\sigma$ and has $\btheta = \{ \forall i,j,k,l : w^{(1)}_{i \rightarrow j}, w^{(2)}_{j \rightarrow k}, w^{(3)}_{k \rightarrow l}  \}$ with biases omitted. For a sample and its true target variable $(\xa, \ya)$, $f(\xa )$ can be  calculated as follows

\begin{align*}
		h_j^{(1)} &= \sum_i w_{i \rightarrow j}^{(1)} x_i^{(\alpha)} & a_j^{(1)} &= \sigma (h_j^{(1)})	\\
		h_k^{(2)} &= \sum_j w_{j \rightarrow k}^{(2)} a_j^{(1)}  & a_k^{(2)} &= \sigma (h_k^{(2)})	 \\
		h_l^{(3)} &= \sum_k w_{k \rightarrow l}^{(3)} a_k^{(2)} & a_l^{(3)} &= \sigma (h_l^{(3)})	 \\
		f(\x) &= [ a_1^{(3)}, \dots, a_L^{(3)}  ]^T  & J &= \frac{1}{p} \sum_{\alpha = 1}^{p} L(f(\xa), \ya)	
\end{align*}

The gradients can be then computed by recursively applying chain rule from the last to the first layer, yields \textit{backpropagation} algorithm.
\begin{align}
	\frac{\partial l(f(\xa), \ya)  }{\partial w_{k \rightarrow l}^{(3)} } &= 	\frac{\partial l(f(\xa), \ya) }{\partial a_{l}^{(3)} }  \frac{\partial a_{l}^{(3)} }{\partial w_{k \rightarrow l}^{(3)} }  	\\
		&= 	\underbrace{\frac{\partial l(f(\xa), \ya) }{\partial a_{l}^{(3)} } \sigma'(h_l^{(3)})}_{ \delta_l^{(3)}} a_{k}^{(2)} 	\\
	\frac{\partial l(f(\xa), \ya)  }{\partial w_{j \rightarrow k}^{(2)} } 
		&=  \sum_{l' = 1}^{L} 	\frac{\partial l(f(\xa), \ya) }{\partial a_{l'}^{(3)} } \frac{\partial a_{l'}^{(3)}}{\partial w_{j \rightarrow k}^{(2)}} \\
		&= \sum_{l' = 1}^{L} 	\frac{\partial l(f(\xa), \ya) }{\partial a_{l'}^{(3)} } \sigma'(h_{l'}^{(3)})  \frac{\partial h_{l'}^{(3)} }{\partial w_{j \rightarrow k}^{(2)}} \\
		&= \sum_{l' = 1}^{L} 	\delta_{l'}^{(3)}  w_{k \rightarrow l'}^{(3)} \frac{\partial a_{k}^{(2)} }{\partial w_{j \rightarrow k}^{(2)}} \\
		&= a_{j}^{(1)}  \underbrace{\sigma'(h_{k}^{(2)}) \sum_{l' = 1}^{L} 	\delta_{l'}^{(3)} w_{k \rightarrow l'}^{(3)}}_{\delta_{k}^{(2)}}  \\
	\frac{\partial l(f(\xa), \ya)  }{\partial w_{i \rightarrow j}^{(1)} } &=  x_i  \sigma'(h_{j}^{(1)}) \sum_{k' = 1}^{K} 	\delta_{k'}^{(2)} w_{j \rightarrow k'}^{(2)} 
\end{align}
As shown in the derivations above, backpropagation allows us to efficiently compute the gradients by reusing already calculated gradients from the later layer, for example $\delta_l^{(3)}, 	\delta_{k}^{(2)}$. Moreover, these reused quantities can be also interpreted as error propagated to responsible neurons.

In practice, because the training set usually contains several thousand samples, the gradient update in  (\ref{eq:gradient_update}) would require significant amount of computation to update just one step, not to mention that it could also result in small gradient update step leading to slow progress towards a desire objective performance. Therefore, the training data $D$ is equally divided into batches  $\widetilde{D}_i$  and perform the gradient update for every $\widetilde{D}_i$. For example, the size of $\widetilde{D}_i$ is usually chosen between 32 and 512 samples. This is referred  as \textit{mini-batch gradient descent}.

Lastly, because noise in training data and potentially highly non-smooth of the cost function, learning rate $\lambda$ has great influential on the training process. More precisely, it should not be too small or too large. This requires some effort and experience in order to get the value right. Some work have proposed alternative update rules aiming to make the training process more stable. For example,  \textit{Adaptive Moment Estimation}(Adam)\cite{KingmaAdamMethodStochastic2014}  uses an adaptive learning rate  and incorporates accumulated direction and speed of the previous gradients(\textit{momentum}) into the adjustment, hence more consistent gradient and fast convergence. Other similar proposals are RMSProp\cite{TielemanLectureRmsPropDivide2012} and Adadelta\cite{ZeilerADADELTAAdaptiveLearning2012}.


\subsection{Convolutional Neural Networks} \label{sec:conv}
Convolutional neural networks(CNN) refer to neural networks that employ convolutional operators to process input instead of fully-connected layers(weighted sum). Typically, a convolutional operator is followed by a pooling operator. Using this convolutional and pooling operators allows the neural network to extract hierarchical features that are spatially invariant \cite{ZeilerVisualizingUnderstandingConvolutional2013}. Hence, these type of layers increases predictive capability of NN, while using fewer parameters than traditional fully-connected layers.

%\afterpage{
\begin{figure}[!hbt]
    \begin{center}
		\includegraphics[width=0.5\textwidth]{sketch/cnn_hierachical_features}
		\caption{Hierarchical features learned by a CNN.}
		\label{fig:conv_intuition}
		\small{Source : \cite{LeeConvolutionalDeepBelief2009}}
	\end{center}
\end{figure}


\addfigure{\ref{fig:conv_intuition}} illustrates hierarchical structures that neurons in each layer of a CNN learn to detect. More precisely, this example shows that neurons in the first learn to detect low level features, such as edges, and neurons in middle layer then use knowledge to detect higher level features, for example, nose, mouth or eyes.


%\afterpage{
\begin{figure}[!hbt]
    \begin{center}
		\includegraphics[width=0.8\textwidth]{lenet}
		\caption{LeNet-5 architecture for a digits recognition task.}
		\label{fig:lenet}
		\small{ Source : \cite{LeCunGradientBasedLearningApplied2001} }
	\end{center}
\end{figure}


Since \cite{LeCunGradientBasedLearningApplied2001} proposed LeNet-5, shown in \addfigure{\ref{fig:lenet}}, and successfully applied it to handwritten recognition problems, CNN have become the first choice of architectures in many domains. Particularly, in computer vision, CNN are the major component of state-of-the-art results in various contests. Such successful results are :  AlexNet\cite{KrizhevskyImageNetClassificationDeep2012} that archive  remarkable results on  ImageNet Large-Scale Visual Recognition Challenge 2012(ILSVRC 2012) followed by the achievement of VGG\cite{SimonyanVeryDeepConvolutional2014} and GoogleLenet \cite{SzegedyGoingDeeperConvolutions2014} architecture in ILSVRC 2014 and ResNet\cite{HeDeepResidualLearning2015} in ILSVRC 2015.



\subsection{Recurrent Neural Networks}
Recurrent neural networks(RNNs) are neural networks whose outputs are repeatedly incorporated back into its next computation. \addfigure{\ref{fig:rnn_unfold}} illustrates this idea of recurrent computation by unfolding RNN into computation steps. Let's consider $\x$ a sequence of $\{ x_1, \dots, x_T \}$.  At step $t$, RNN takes $r_{t-1}$ and $x_{t}$ to compute $r_{t}$ and $\hat{y}_t$. This recurrent connections can be interpreted as accumulating information from the past, hence RNN are well suit to process sequential data, with no limitation in length . Natural Language Processing(NLP) and Machine Translation(MT) are some of the fields that RNN are widely applied to.


\begin{figure}[!hbt]
\centering
\includegraphics[width=0.8\textwidth]{sketch/rnn_unfold}
\caption{Unfolded RNN Structure}
\small{Inspired by a figure in \cite{OlahUnderstandingLSTMNetworks2015} }
\label{fig:rnn_unfold} 
\end{figure}


\subsubsection{Backpropagation Through Time}
As the number of computation steps in RNN is depend on the length of samples, which can be different in principle, one needs to organize data in such a way that samples in the same batch have the same  length of computation before training a RNN. As a result, training RNNs can be viewed as training a feedforward neural network with a certain depth of layers, hence backpropagation can be readily applied. In this case, the difference between NN and RNN is the fact that variables are shared the same across layers.

Consider again RNN in \addfigure{\ref{fig:rnn_unfold}} with $\x = \{x_1, \dots, x_T \}$ and $r_0 = 0$. Assume that only $\hat{y}_T$ determines the value of the loss function. The computations are also defined as follows 
\begin{align}
	h_1 &= w_{rx} x_1 + w_{rr} r_0 & r_1 &= \sigma(h_1) \label{eq:naive_r} \\
	h_2 &= w_{rx} x_2 + w_{rr} r_1 &  r_2 &= \sigma(h_2) \\
	& \vdots & \vdots \\
	h_{T-1} &= w_{rx} x_{T-1} + w_{rr} r_{T-2} &  r_{T-1} &= \sigma(h_{T-1}) \\
	\hat{y} &= \sigma(w_{yx} x_T   + w_{yr} r_{T-1})
\end{align}

Therefore, the gradients can be computed by 
\begin{align}
	\frac{\partial l}{\partial w_{yx}} &= \sigma'(w_{yx} x_T   + w_{yr} r_{T-1}) x_T \\
	\frac{\partial l}{\partial w_{yr}} &= \sigma'(w_{yx} x_T   + w_{yr} r_{T-1}) r_{T-1} \\
	\frac{\partial l}{\partial w_{rx}} &= 	w_{yr} \sigma'(w_{yx} x_T   + w_{yr} r_{T-1}) \frac{\partial r_{T-1}}{\partial w_{rx}} \\
	&= w_{yr} \sigma'(w_{yx} x_T   + w_{yr} r_{T-1})  \Bigg[ \sigma'(h_{T-1}) \bigg( x_{T-1} + w_{rr}  \frac{\partial r_{T-2}}{\partial w_{rx}} \bigg) \Bigg]  \label{eq:gradient_wrr}  \\
	\frac{\partial l}{\partial w_{rr}} &= w_{yr} \sigma'(w_{yx} x_T   + w_{yr} r_{T-1})  \frac{\partial r_{T-1}}{\partial w_{rr}}  \\
	&= w_{yr} \sigma'(w_{yx} x_T   + w_{yr} r_{T-1})  \Bigg[ \sigma'(h_{T-1}) \bigg( \frac{\partial r_{T-2}}{\partial w_{rr}} \bigg) \Bigg] \\
\end{align}
However, as the computations unfolded, we can see that there are 2 problems that might happen to the gradients of the shared parameters $w_{rx}$ and $ w_{rr}$, namely
\begin{itemize}
	\item \textit{Exploding gradient} happens if the gradient is derived from shared weights, for example $w_{rr}$ in  (\ref{eq:gradient_wrr}), whose absolute value is greater than one. The recursive multiplication will result in a large value of the gradient leading to unreliable training. \cite{PascanuUnderstandingexplodinggradient2012} have proposed \textit{gradient clipping} to alleviate the problem.
	\item \textit{Varnishing gradient}  in contrast occurs when the values are smaller than one yielding  near zero gradients. This issue leads to slow learning, hence RNN would require enormous of time to learn, especially long term dependencies. The next section discusses techniques to mitigate this problem.
\end{itemize}


\subsubsection{Long Short-Term Memory and Gated RNNs}
Varnishing gradient is a major problem that causes RNN to learn long term memories with slow progress. This is because the computation of recurrent connections are constructed. In particular, as in (\ref{eq:naive_r}), standard RNN compute those connections with weighted sum at every step $t$ leading to recursive multiplication terms in the gradient computations.

Alternatively, \cite{HochreiterLongshorttermmemory1997} proposed \textit{Long Short-Term Memory}(LSTM) architecture that employs a gating mechanism and additive updates in the computation of the recurrent connections. This mechanism decreases number of damping factors involved in the gradients, hence enabling better learning long term memories.

As shown in \addfigure{\ref{fig:lstm_structure}}, LSTM utilizes 3 gates, namely input $i_g$, forget $f_g$ and $o_g$ output gate, to control  information flow through LSTM cell. More precisely, $i_g$ and $f_g$ decides how to accumulate information from the previous cell state $C_{t-1}$, and the cell state computed from previous output $h_{t-1}$ and current input $x_t$, into a new cell state $C_t$. On the other hand, $o_g$ determines to leakage of the information from $C_t$ to outside $h_t$. Formally, 
\begin{align}
	i_g &= \sigma( w_{ix} x_t + w_{ih} h_{t-1} )  &  	f_g &= \sigma( w_{fx} x_t + w_{fh} h_{t-1} )\\
	o_g &= \sigma( w_{ox} x_t + w_{oh} h_{t-1} ) & \widetilde{C}_t &= \tanh(w_{cx} x_t + w_{ch} h_t) \\
	C_t &= f_g \otimes C_{t-1} + i_g  \otimes  \widetilde{C}_t & h_{t} &= o_g \otimes \tanh(C_t)
\end{align}


\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{sketch/lstm}
\caption{LSTM Structure} 

\label{fig:lstm_structure} 
\end{figure}

Since the work was published, LSTM has successfully contributed to many state-of-the-art results in machine translation and NLP\cite{MelisStateArtEvaluation2018}. \cite{GreffLSTMsearchspace2017} demonstrated that the forget and output gate are the crucial parts of LSTM.  \cite{ChoLearningPhraseRepresentations2014a} proposed \textit{Gated Recurrent Unit}(GRU) that employs only 2 gates, however \cite{Jozefowiczempiricalexplorationrecurrent2015a} conducted several benchmarking tasks and found no significant difference in performance between LSTM and GRU. 