\chapter{Conclusion}
\label{cha:chapter5}
%\section{Summary}
We have provided extensive experiments towards explaining RNN decisions. Our experiments are artificially designed such that qualitative and quantitative evaluations can be done accordingly.  The results demonstrate that the architecture of RNNs has a considerable impact on the quality of explanation. More precisely, we found that deeper and gated architectures improve the explainability of RNNs significantly. 

Moreover, the level of influence from the architecture configuration is different for each explanation technique. Based on our quantitative evaluations, deep Taylor decomposition(DTD) and Layer-wise Relevance Propagation(LRP) are more influenced by the architecture than sensitivity analysis(SA) and guided backprop(GB).  Training configuration is also another influential factor that could affect the quality of explanation. In particular, for a specific setting, training with stationary dropout shows a slight improvement in visual quality although our quantitative measurements do not capture the impact.

More importantly, it is worth noting that we consider ConvR-LSTM-SD as the most explainable architecture in this thesis. In particular, we achieve decent explanation heatmaps when explaining it via $\lrpp$ without negative relevance considered. As a reminder, this result is shown on \addfigure{\ref{fig:heatmap_msc_convrlstm_pos_rel}}.

Lastly, we would like to argue further that the quality of RNN explanation should be considered into two aspects, namely fine-grained and coarse-grained.  The fine-grained aspect describes whether the explanation of each input from a sequence is sound. In case of image related applications, it is already shown in the literature that this aspect can be improved by employing convolutional and pooling layers. Our ConvDeep experiments confirm this in RNN setting. On the other hand, the coarse-grained aspect tells us whether RNNs can adequately propagate relevance quantities to the right time steps in a sequence. Our experiments strongly suggest that using LSTM-type architecture is the key to improve the quality of explanation in this aspect. Therefore, RNNs need to satisfy these two aspects to establish great explainability.



\section{Challenges}
We have encountered several challenges while working on the thesis. The first challenge is regarding evaluation criteria. In particular, it is quite challenging to evaluate the quality of explanations when we do not have the ground truth information available. To mitigate this problem, we constructed artificial sequence classification problems such that we know parts of input that are relevant to the objective.  Secondly, we have experienced that initialization scheme of weights might also affect the quality of explanations although it does not affect the objective performance. In fact, some architectures have worse results if weights are not initialized with the $1/\sqrt{N_{in}}$ scheme.

Lastly, because we only relied on basic frameworks, such as TensorFlow, and implemented most of the code ourselves, we have found that implementing neural network systems is more challenging than traditional software development in a sense that we do not have a good way to verify the correctness of the code. Given the reason, we found that conservation property is practically useful because it allows us to write unit tests that automatically check the implementation This does not only allow us to validate new deployments quickly, but it also makes sure that there will not be any systematic mistake in the implementation of LRP and DTD explanation for new architectures.

%hyper parameters... 

\section{Future work}
Despite results from our extensive experiments, we still consider our experimental setting limited, for example, we fixed the sequence length.  Hence, one of future tasks would be to generalize and apply our work to a broader context. In particular,  applying the experiments on more diverse datasets and sequence lengths could be the first straightforward extension. Because of the popularity of RNN in NLP domain, problems in this direction, such as text classification or sentiment analysis, are worth experimenting. 

As discussed earlier, quantifying the quality of explanation from RNN is challenging in several aspects. We believe establishing a better quantitative evaluation methodology is another relevant future work.

%because we do not have any reliable measurement to measure local quality corresponding to each input. This leads to inconsistent results between qualitative and quantitative evaluation. Hence, another possible study could be evaluation methodology for quantifying explanation heatmaps.
%
%ConsequeTntly, some of our quantitative results were slight different from what we have seen from comparison of actual explanation heatmaps. 
%
%
%Another possible study is about evaluation methodology for quantifying explanation heatmaps. In particular, we found that quantifying RNN explanation is challening because we do not have any reliable measurement to measure local quality corresponding to each input. Hence, some of our quantitative results were slight different from what we have seen from comparison of actual explanation heatmaps. 
%
%% We found that cosine similarity is somewhat limited because it is computed from global explanation 
%%
%%discards local information in explanation heatmap, hence the score is do  the quality of  could not capture whether local expalanation is good.
%%
%% study.