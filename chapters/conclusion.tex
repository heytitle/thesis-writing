\chapter{Conclusion}
\label{cha:chapter5}
%\section{Summary}
%\todo{summary}
 Unlike regular neural network, the quality of RNN explanation need to be considered into 2 aspects, namely local and global aspect. The local aspect describes whether explanation of each input sound. In case of image related applications, this can be improved by employing convolutional and pooling layer. On the other hand, the global aspect tells us whether RNN can properly propagate relevance quantities to the right inputs. Local explanation can not have good quality if RNN lacks capability in the global aspect.

Our experiments are artificially designed such that qualitative and quantitative evaluations can be done accordingly.  The results demonstrates that the relevance propagating capability of RNN is considerably depend on the architecture of the RNN itself. In particular, deeper architecture and employing gating units significantly allows RNN to better propagate relevance scores to corresponding input. 


Moreover, the level of  influence from the architecture configuration is different for each explanation technique. In particular, based on our quantitative measurements, deep Taylor decomposition and Layer-Wise Relevance Propagation(LRP) are methods the most influenced by the architecture than sensitivity analysis(SA) and guided backprop(GB).  Training configuration is also another factor that could affect quality of explantation. In particular, training with stationary dropout shows slight improvement on visual quality of explanation although the impact is not captured by our quantitative measurement.

Lastly, it is worth noting that we consider ConvR-LSTM-SD as the most explainable architecture in this thesis. In particular, we achieve decent explanation heatmaps when explaining it via $\lrpp$ without negative relevance considered. As a reminder, this result is shown on \addfigure{\ref{fig:heatmap_msc_convrlstm_pos_rel}}.

\section{Challenges}
Throughout the time working on the thesis, we have encountered several challenges. The first challenge is about evaluations. In particular, it is quite challenging to evaluate the quality of explanations when we do not have ground truth information available. This challenge led us to artificially construct the majority sample sequence classification problem to mitigate the problem.  Secondly, we have also experienced that initialization scheme of weights and biases is also another factor that could affect the quality of explanations although it would not affect the objective performance.


Lastly, because we used only basic frameworks, such as TensorFlow, and implemented most of the code ourselves, we have found that implementing neural network systems is more difficult than traditional software development in a sense that we do not have a good way to properly verify the correctness of the code. In this aspect, we have found that properties, such as conservation property, are extremely useful because it allows us to write unit tests that automatically verify the implementation during the development. This is at least making sure that we will not make any systematic mistake in the integral part of the implementation of LRP and DTD explanation.

%hyper parameters... 

\section{Future work}
Despite extensive results from our experiments, we still consider our experimental setting somewhat limited. Hence, it would be better if we generalize and apply our work to broader setting. One possible future work would be applying the experiments on more diverge dataset and sequence length. Due to that fact that RNN are widely used in NLP, problems in this direction, such as text classification or sentiment analysis, are also worth experimenting. 

As discussed earlier that,  quantifying RNN explanation is challenging because we do not have any reliable measurement to measure local quality corresponding to each input. This leads to inconsistent results between qualitative and quantitative evaluation. Hence, another possible study could be evaluation methodology for quantifying explanation heatmaps.
%
%ConsequeTntly, some of our quantitative results were slight different from what we have seen from comparison of actual explanation heatmaps. 
%
%
%Another possible study is about evaluation methodology for quantifying explanation heatmaps. In particular, we found that quantifying RNN explanation is challening because we do not have any reliable measurement to measure local quality corresponding to each input. Hence, some of our quantitative results were slight different from what we have seen from comparison of actual explanation heatmaps. 
%
%% We found that cosine similarity is somewhat limited because it is computed from global explanation 
%%
%%discards local information in explanation heatmap, hence the score is do  the quality of  could not capture whether local expalanation is good.
%%
%% study.