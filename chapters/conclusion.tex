\chapter{Conclusion}
\label{cha:chapter5}
%\section{Summary}
We have provided extensive experiments towards explaining RNN predictions. Our experiments are artificially designed such that qualitative and quantitative evaluations can be done accordingly.  The results demonstrate that the architecture of RNNs has a considerable impact on the quality of explanation. More precisely, we found that deeper and LSTM-type architectures have a great level of explainability.

Moreover, the level of influence from the RNN structure to the quality of explanation is different for each explanation technique. Based on our quantitative evaluations, the deep Taylor decomposition (DTD) and Layer-wise Relevance Propagation (LRP) techniques are more influenced by the RNN architecture than the sensitivity analysis (SA) and guided backprop (GB) methods.  Training configuration is also another influential factor that can affect the quality of explanation. In particular, for a certain architecture, training with stationary dropout shows a slight improvement in visual quality although our quantitative measurements do not capture the impact.

More importantly, it is worth mentioning that we consider ConvR-LSTM-SD as the most explainable architecture in this thesis. In particular, we achieve the decent explanation heatmaps when explaining it via $\lrpp$ without negative relevance considered. As a reminder, this heatmaps are shown in \addfigure{\ref{fig:heatmap_msc_convrlstm_pos_rel}}.

Lastly, we would like to argue further that the quality of RNN explanation should be considered in two aspects, namely fine-grained and coarse-grained.  The fine-grained aspect describes whether the explanation of each input from a sequence is sound. In case of image related applications, it is already shown in the literature that this aspect can be improved by employing convolutional and pooling layers. Our ConvDeep experiments confirm this in the RNN setting. On the other hand, the coarse-grained aspect tells us whether RNNs can adequately propagate relevance quantities to the right time steps in a sequence. Our experiments strongly suggest that using LSTM-type architecture is the key to improve the quality of explanation in this aspect. Therefore, RNNs need to satisfy these two aspects to establish great explainability.



\section{Challenges}
We have encountered several challenges while working on the thesis. Firstly, it is quite challenging to evaluate the quality of explanations when we do not have the ground truth information available. To mitigate this problem, we constructed artificial sequence classification problems such that we know parts of the input that are relevant to the objective.  Secondly, we have experienced that the initialization scheme of weights might also affect the quality of explanations although it does not affect the objective performance. In fact, some of the introduced architectures had worse explanations when weights were not initialized with the $1/\sqrt{N_{in}}$ scheme.

Lastly, because we only relied on basic frameworks, such as TensorFlow, and implemented most of the code ourselves, we found that implementing neural network systems is more challenging than traditional software development in a sense that we do not have a good way to verify the correctness of the code. Given that reason, we discovered that the conservation property is practically useful because it allows us to write unit tests that automatically check the implementation. This does not only enable us to validate new developments quickly, but it also makes sure that there will not be any systematic mistake in the implementation of LRP and DTD explanations for new architectures.

%hyper parameters... 

\section{Future work}
Despite results from our extensive experiments, we still consider our experimental setting limited, for example, we experimented using only a sequence length in some experiments.  Hence, one of future tasks would be to generalize and apply our work to a broader context. In particular,  applying the experiments on more diverse datasets and sequence lengths could be the first straightforward extension. Because of the popularity of RNNs in the NLP community, problems in this direction, such as text classification or sentiment analysis, are worth experimenting. 

As discussed earlier, quantifying the quality of RNN explanations is challenging in several aspects. We believe establishing a better quantitative evaluation methodology is another relevant future work.