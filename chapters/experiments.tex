\chapter{Experiments}
\label{cha:chapter4}


\section{General Setting}\label{sec:setup}
 
 We use \textit{Adaptive Moment Estimation(Adam)}\cite{KingmaAdamMethodStochastic2014} to train models and initialize weights $w_{ij} \in \patmatrix{W}$ and biases $b_{j} \in \patvector{b}$ as follows
\begin{align*}
	w_{ij} &\sim \Psi( \mu, \sigma, [-2\sigma, 2\sigma]) \\
	b_{j} &= \ln(e^{0.01} - 1)
\end{align*}
where $\Psi(\cdot)$ denotes \textit{truncated normal distribution} where $\mathbb{P}(|w_{ij}| > 2\sigma) = 0$. Precisely, we use $\mu=0$ and $\sigma = 1/\sqrt{|\boldsymbol{a}|}$ where $|\boldsymbol{a}|$ is the number of neurons in previous layer.

The activations of neurons in each layer $j$, denoted as $\patvector{a}^{(j)}$, are computed using 
\begin{align*}
\patvector{h}^{(j)}  &=  	(\patmatrix{W}_{ i \rightarrow j })^T \patvector{a}^{(i)} - \sigma_{s}(\patvector{b_j}) \\
	\patvector{a}^{(j)}  &=  	\sigma_{r} (	\patvector{h}^{(j)} )
\end{align*}

where $\sigma_r(\cdot)$ and $\sigma_s(\cdot)$ are \textit{ReLU} and \textit{softplus} function respectively and applied element-wise to elements in the vectors.

 \begin{figure}[!hbt]
\centering
\includegraphics[width=0.5\textwidth]{relu_softplus}
\caption{ReLU and Softplus function} 
\label{fig:relu_softplus}
\end{figure}


The reason of using softplus function for the bias term is due to the non positive bias assumption of DTD. Secondly, the continuity of softplus function is another reason. This property allows the bias term to be more flexibly adjusted through back-propagation than using ReLU. With this setting, the initial value of bias term  $\sigma_{s}(b_j)$ is then 0.01.


\begin{table}[!htb]
\centering
\begin{tabular}{l|r}
\textbf{Hyperparameter} & \multicolumn{1}{l}{\textbf{Value}} \\ \hline
Optimizer               & Adam                               \\
Epoch     & 100                                \\
Dropout Probability     & 0.2                               \\
Batch size              & 50                                
\end{tabular}
\caption{Summary of hyperparameter.}
\label{tab:hyper_summary}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{ll}
\multicolumn{1}{l|}{\textbf{Dataset}} & \textbf{Minimum Accuracy} \\ \hline
\multicolumn{1}{l|}{MNIST}            & \multicolumn{1}{r}{98.00\%}  \\
\multicolumn{1}{l|}{FashionMNIST}    & \multicolumn{1}{r}{85.00\%}  \\
%\multicolumn{1}{l|}{UFI Cropped}                                       &                         \dots
\end{tabular}
\caption{Minimum classification accuracy for models to be considered.}
\label{tab:min_acc}
\end{table}



Dropout technique \cite{SrivastavaDropoutSimpleWay2014} is applied to activations of every fully-connected layer, unless stated otherwise. We use the dropout probability at 0.2. We train models with batch size 50 for 100 epochs. Table \ref{tab:hyper_summary} summaries the setting of hyperparameters. On the other hand, learning rate is not globally fixed and left adjustable per architecture: we use the value between 0.0001 and 0.0005. Based on literature surveys, Table \ref{tab:min_acc} shows minimum accuracy of each dataset. Models considered in the following experiments need to satisfy this requirement.  Numbers of neurons in each layer were carefully chosen such that every architecture has similar number of trainable variables. More precise configuration will be discussed separately in each experiment. 

Consider RNN with parameters $\boldsymbol{\theta} = \{ \patmatrix{W}, \boldsymbol{b} \}$. Assume that $g_r$ and $g_{f}$ are functions that the RNN uses to compute recurrent input $\patvector{r}_{t+1}$ and $f(\x)$ respectively. For a classification problem with $K$ classes, the calculations can be  roughly summarized as follows
 \begin{align}
 	\patvector{r}_{t+1} &= g_r(\patvector{\theta}, \patvector{x_t}, \patvector{r_t}) \\
 	 &\ \ \vdots\\
f(\x) &= g_{f}(\patvector{\theta}, \patvector{x}_{T},  \patvector{r}_{T}) \\
 	\patvector{\hat{y}} &= \text{softmax}(f(\x)),
 \end{align}
 where $t \in \{1, \dots, T\}$, $(\x_t \in \x)_1^{T}$ are input corresponding to  step $t$, $\patvector{r}_0 = \patvector{0}$, and $\patvector{\hat{y}} \in \mathbb{R}^K$ are the vector of class probabilities. To compute explanation or relevance heatmap of $\x$, denoted as $R(\x)$, we take $z^* \in f(\x)$ that is corresponding to the true target class, instead of the predicted class.  Because DTD and LRP method are primarily  based on distributing positive relevance, we also introduce a constant input, whose value is zero, to the softmax function to force the network building positive relevance, $z^* \in \mathbb{R}^+$. Mathematically, this constant does not affect the training procedure.

Our implementation is written in Python and based on TensorFlow\cite{AbadiTensorFlowLargeScaleMachine2016}. It is publicly available on Github\footnote{\url{https://github.com/heytitle/thesis-designing-recurrent-neural-networks-for-explainability/releases/tag/release-final}}.  We run our experiments either on a GeForce GTX 1080 provided by TUB ML group or AWS's p2.xlarge\footnote{\url{https://aws.amazon.com/ec2/instance-types/p2/}} instance. With this setting, each model approximately takes 2 hour to train.


% \todo{computational resource}
% \todo{Tensoflow Python and Repo}

 
 % 
%Traditionally, number of neurons in each layer ($n^{(l)}$) is  another hyperparameter that we can adjust. However, as the goal is to compare relevance heatmaps from different architectures, those numbers are fixed and chosen in such a way that total number of variables in each architecture are equivalent. \addfigure{\ref{fig:neuron_numbers}} illustrates the details of the settings.
%
%\begin{figure}[!htb]
%\centering
%\includegraphics[width=\textwidth]{sketch/neuron_numbers}
%\caption{Number of neurons in each layer for each cell architecture}
%\label{fig:neuron_numbers}
%\end{figure}
%
%
%\begin{itemize}
%	\item \textbf{Shallow Cell} 
%$$\{ n^{(4)}\} = \{ 768 \}$$
%	\item \textbf{Deep Cell} 
%$$\{ n^{(1')}, n^{(4)}, n^{(4')} \} = \{ 512, 256, 64 \}$$
%	\item \textbf{DeepV2 Cell} 
%$$\{ n^{(1')}, n^{(1")}, n^{(4)}, n^{(4')} \} = \{ 512, 256, 128, 64 \}$$
%	\item \textbf{ConvDeep Cell} : 
%\begin{align*}
%	\{ n^{(C1)}, n^{(P1)} \} &= \{ CONV(5\text{x}5, 24), POOL(2\text{x}2) \} \\
%		\{ n^{(C2)}, n^{(P2)} \} &= \{ CONV(3\text{x}3, 48), POOL(2\text{x}2) \} \\
%			\{  n^{(4)}, n^{(4')} \} &= \{ 256, 128 \}
%\end{align*}
%where $CONV(x,y)$ is a convolutional operator with $y$ filters whose kernel size is $\mathbb{R}^{x}$. Similarly, $POOL(x)$ is a pooling operator  with kernel size $\mathbb{R}^{x}$.
%
%
%\end{itemize}
%
%Noting that, $n^{(5)}$ is set at 128 for all architectures and 0 when the sequence length of the problem is 1. $n^{(6)}$ is equal to the number of categories of a problem, for example $n^{(6)} = 10 $ MNIST. Table \ref{tab:variable_architecture} shows the total numbers of variables in details.

%\renewcommand{\arraystretch}{1.2}
%\begin{table}[h]
%\centering
%\begin{tabular}{l|c|c|c|}
%\cline{2-4}
%                                                 & \multicolumn{3}{c|}{\textbf{Sequence Length}} \\ \hline
%\multicolumn{1}{|l|}{\textbf{Cell Architecture}} & 1         & 4         & 7         \\ \hline
%\multicolumn{1}{|l|}{\rnncell{Shallow}}                    & 610570    & 355722    & 291210      \\ \hline
%\multicolumn{1}{|l|}{\rnncell{Deep}}                       & 550346    & 314954    & 271946      \\ \hline
%%\multicolumn{1}{|l|}{\rnncell{DeepV2}}                    & 575050    & 306890    & 263882      \\ \hline
%%\multicolumn{1}{|l|}{\rnncell{ConvDeep}}                   & 647594    & 283178    & 197162      \\ \hline
%\end{tabular}
%\caption{Total variables in each architecture and sequence length}
%\label{tab:variable_architecture}
%\end{table}


 

%\clearpage
\section{Experiment 1 : Sequence Classification}
\label{sec:exp1}

\subsection{Problem Formulation}
We consider this experiment as a preliminary study. Here, we constructed an artificial classification problem in which each image sample $\x$ is column-wise split into a sequence of non-overlapping $(\x_t)_{t=1}^{T}$. The RNN classifier needs to summarize information from the sequence $(\x_t)_{t=1}^{T}$ to determine what is the class of $\x$.   Using image allows us to conveniently inspect produced explanations.

\addfigure{\ref{fig:artificial_problem}} illustrates the setting. As shown in the figure, a MNIST sample $ \patvector{x} \in \mathbb{R}^{28,28}$ is divided to a sequence of $( \patvector{x}_t \in   \mathbb{R}^{28,7} )_{t=1} ^ 4$. At time step $t$, $\patvector{x}_t$ is presented to the RNN classifier yielding recurrent input $\patvector{r}_{t+1}$ for the next step. For the last step $T = 4$, the RNN classifier computes $f(\x) \in \mathbb{R}^{10}$ and the class probabilities accordingly.


 \begin{figure}[!hbt]
		\centering
		\includegraphics[width=\textwidth]{sketch/artificial_problem_with_rel}
		\caption{RNN Sequence classifier and decision explanation} 
		\label{fig:artificial_problem}
\end{figure}


\begin{figure}[!htb]
\centering

\subfloat[Shallow architecture\label{fig:shallow_arch}]{%
       \includegraphics[width=0.48\textwidth]{sketch/shallow_arch}
     }
     \hfill
     \subfloat[Deep architecture\label{fig:deep_arch}]{%
       \includegraphics[width=0.48\textwidth]{sketch/deep_arch}
     }
\patcaption{Shallow and Deep architecture}{with number of neurons at each layer depicted.}
\end{figure}

In this experiment, we are going to  consider 2 RNN architectures, namely

\begin{enumerate}
	\item \textbf{Shallow architecture} \\
		As shown in \addfigure{\ref{fig:shallow_arch}}, the \rnncell{Shallow} architecture first concatenates  input $\patvector{x}_t$  and recurrent input $\patvector{r}_t$ at layer \circled{3} as one vector before computing activations of layer \circled{4}, denoted as $\patvector{a}_t^{(4)}$. Then,  the next recurrent input $\patvector{r}_{t+1}$ \circled{5}	 is derived from $\patvector{a}_t^{(4)}$. In the last step $T$, $f(\x)$ is computed from $\patvector{a}^{(4)}_{T}$ and applied to softmax function to compute class probabilities $\patvector{\hat{y}}$. 
		
Because activations coming to \circled{4} are from different domains, $\x_t \in [-1, 1]$ and $\patvector{r}_t \in [0, \infty) $, a special propagation rule is needed in order to apply DTD.  Denoting $i$ and $j$ the pixels in $\x_t$ and activations in $\patvector{r}_t$ respectively and $k$ the corresponding neuron in \circled{4}, the propagation rule is defined as 
		
\begin{align}
	R_{t, i} &= \sum_k \frac{(x_{t, i} w_{ik} - l_i w_{ik}^+ - h_i w_{ik}^-) R_{t,k}}{z_{t,k}} \\	
     R_{t, j} &= \sum_k \frac{r_{t, j} w_{jk}^+ R_{t,k}}{z_{t,k}}
\end{align}
where $z_{t,k} = \sum_j r_{t,j} w_{jk}^+ + \sum_i x_{t,i} w_{ik} - l_i w_{ik}^+ - h_i w_{ik}^-$ is the normalization term.

	\item \textbf{Deep architecture} \\
		\addfigure{\ref{fig:deep_arch}} illustrates the configuration of this architecture. Unlike the Shallow architecture, the Deep architecture has 2 more layers in the beginning, namely \circled{1'} and \circled{4'}.  The improvement would allow \circled{1'} to properly learn representations from input and \circled{4} to efficiently combine information from the past and current input. Hence, this then enables \circled{4'} to compute more fine-grained decision probabilities leading to better overall quality of explanation.
\end{enumerate}

\renewcommand{\arraystretch}{1.5}
\begin{table}[h]
\centering
\begin{tabular}{cc|c|c|}
\cline{3-4}
& & \multicolumn{2}{c|}{\textbf{No. trainable variables}}                                                                \\ \hline
\multicolumn{1}{|c|}{\textbf{T}}               & \multicolumn{1}{c|}{\textbf{Dim. of $\x_t$}} & \multicolumn{1}{c|}{\textbf{Shallow}} & \multicolumn{1}{c|}{\textbf{Deep}}  \\ \hline
\multicolumn{1}{|c|}{1} & $\mathbb{R}^{28,28}$ & 269,322  &  271,338 \\
\multicolumn{1}{|c|}{4} & $\mathbb{R}^{28,7}$ & 184,330 & 153,578 \\
\multicolumn{1}{|c|}{7} & $\mathbb{R}^{28,4}$ & 162,826 & 132,074 \\ \hline

\end{tabular}
\caption{Dimensions of $\patvector{x}_t$ and number of trainable variables in each architecture on different sequence length $T=\{1, 4, 7\}$.}
\label{tab:seq-length}
\end{table}
\renewcommand{\arraystretch}{1}




We experimented with MNIST and FashionMNIST using sequence length $T = \{1, 4, 7\}$.  Table \ref{tab:seq-length} shows dimensions of $\patvector{x}_t$ for different sequence length as well as number of trainable variable in each architecture.

To simplify the writing, we are going to use \textit{\rnncellseq{ARCHITECTURE}{T}} convention to denote a RNN with \textit{ARCHITECTURE} trained on the sequence length \textit{T}. For example, \rnncellseq{Deep}{7} refers to the Deep architecture trained on $(\x_t \in \mathbb{R}^{28,4} )_{t=1}^{7}$.

\subsection{Result}
\label{sec:exp1_result}

\renewcommand{\arraystretch}{1.5}
\begin{table}[]
\centering
\begin{tabular}{cc|c|c|c|}
\cline{2-5}
& \multicolumn{2}{|c|}{\textbf{MNIST}} & \multicolumn{2}{|c|}{\textbf{FashionMNIST}} \\ \hline
\multicolumn{1}{|c|}{$T$}   & \multicolumn{1}{c|}{\textbf{Shallow}} & \multicolumn{1}{c|}{\textbf{Deep}} & \multicolumn{1}{c|}{\textbf{Shallow}} & \multicolumn{1}{c|}{\textbf{Deep}} \\ \hline
\multicolumn{1}{|c|}{1} & 98.11\%   & 98.22\% & 87.93\%  & 89.14\%                           \\
\multicolumn{1}{|c|}{4} & 98.56\% & 98.63\%  & 89.04\%  & 89.43\%                            \\
\multicolumn{1}{|c|}{7} & 98.66\%  & 98.68\% & 89.28\%  & 88.96\%  \\ \hline
\end{tabular}
\caption{Sequence classification accuracy from Shallow and Deep architecture trained with different sequence length.}
\label{tab:mnist_model_acc}
\end{table}
\renewcommand{\arraystretch}{1}



 \begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{sketch/mnist_experiment}
\patcaption{Relevance heatmaps from different explanation techniques applied to Shallow and Deep architecture trained on MNIST with different sequence lengths.}{\heatmapscaleexplain }
\label{fig:mnist_experiment}
\end{figure}


Table \ref{tab:mnist_model_acc} summaries accuracy of the trained models. Both Shallow and Deep architecture have comparable accuracy, hence their explanations can also be compared. \addfigure{\ref{fig:mnist_experiment}} shows relevance heatmaps from Shallow and Deep architecture trained on MNIST.  We can observe similar characteristics of each explanation technique as in \addfigure{\ref{fig:lenet_heatmaps}}. In particular, SA and GB explanations are sparse, while the ones from DTD and $\lrpp$ are more diffuse throughout $\x$. 

\rnncellseq{Shallow}{1}  and \rnncellseq{Deep}{1} have similar relevance heatmaps regard less of explanation methods.  As the sequence length increased, \rnncellseq{Shallow}{4,7} and  \rnncellseq{Deep}{4,7} start producing significantly different relevance heatmaps when explained by DTD and $\lrpp$.  In particular,  \rnncellseq{Shallow}{4,7} 's explanations are mainly concentrated on the right, which associate to input of last time steps, while  \rnncellseq{Deep}{4,7}'s ones are proportionally  highlighted around content area of $\x$. We do not see this effect from SA and GB.


 \begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{sketch/fashion_mnist_experiment}
\patcaption{Relevance heatmaps from different explanation techniques applied to  Shallow and Deep architecture trained on FashionMNIST with different sequence lengths.}{\heatmapscaleexplain}
\label{fig:fashion_mnist_experiment}
\end{figure}

Relevance heatmaps of Shallow and Deep architecture trained on  FashionMNIST  are shown on \addfigure{\ref{fig:fashion_mnist_experiment}}. Similar to the ones from MNIST, we do not see any remarkable difference on SA and GB heatmaps of the two architectures although \rnncellseq{Deep}{4,7} produces slightly more sparse heatmaps than \rnncellseq{Shallow}{4,7}. However, the wrong concentration issue of DTD and LRP seems to appear on both \rnncellseq{Shallow}{4,7}'s and \rnncellseq{Deep}{4,7}'s heatmaps. Nevertheless, we can still observe proper highlight from Deep architecture on some samples. For example, the trouser sample, we can see  that \rnncellseq{Deep}{4,7} architecture manage to distribute high relevance scores to area of the trouser. 

Similar structures of FashionMNIST samples might be one of the reasons why Deep architecture is not able to distribute relevance scores to earlier steps as in MNIST cases. Consider \textit{Shoe} and \textit{Ankle Boot} samples in \addfigure{\ref{fig:fashion_mnist_samples}}. One can see that  their front part are similar and only the heel part that determines the difference between the two categories. This evidence suggests that  more robust feature extractor layer, such as convolution and pooling layer, might be more suitable than the fully-connected one.


 \begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{sketch/class_1_comparison}
\patcaption{Relevance heatmaps of MNIST \textit{Class 1} and FashionMNIST \textit{Class Trouser} samples from \rnncellseq{Shallow}{7} and \rnncellseq{Deep}{7} explained by DTD and $\lrpp$.}{\heatmapscaleexplain }
\label{fig:class_1_comparison}
\end{figure}

\addfigure{\ref{fig:class_1_comparison}} presents explanations of MNIST \textit{Class 1} and FashionMNIST \textit{Class Trouser} samples. These samples were chosen to emphasize the impact of RNN architecture on DTD and LRP explanation. As can be seen from the figure, these samples have $\x_{t'}$ containing actual content  primarily locating at the center, or middle of the sequence. Hence, relevance heatmaps should be highlighted at $\x_{t'}$ and possibly its neighbors.  As expected, we can see \rnncellseq{Deep}{7} produces sound explanations in which the heatmaps have high intensity value where $\x_{t'}$ approximately locate, while \rnncellseq{Shallow}{7} mainly assigns relevance quantities to $\x_{t}$ for $t \approx T$. 

\addfigure{\ref{fig:exp1_dist_plot}} further shows a quantitive evidence of this wrong propagation issue of DTD and $\lrpp$. Here, distributions of relevance scores derived from the methods on \rnncellseq{Shallow}{7} and \rnncellseq{Deep}{7} are plotted across time step $t = \{ 1, \dots, 7 \}$. The distributions are computed from all test samples in MNIST \textit{Class 1} and FashionMNIST \textit{Class Trouser} respectively. The plots also include distribution of pixel  values.  We can see that the relevance distributions from \rnncellseq{Deep}{7} align with the data distributions, while  the distributions from \rnncellseq{Shallow}{7} ones diverge with significant margin. Approximately,  one can see that \rnncellseq{Shallow}{7} distributes more than 90\% of relevance scores to the last 3 steps, namely $\x_5$, $\x_6$ and $\x_7$.


 \begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{sketch/exp1_dist_plot}
\caption{Distribution of pixel intensity, relevance quantities from \rnncellseq{Shallow}{7} and \rnncellseq{Deep}{7} propagated by DTD and $\lrpp$ and averaged over MNIST \textit{Class 1} and FashionMNIST\textit{Class Trouser} test population.} 
\label{fig:exp1_dist_plot}
\end{figure}

\subsection{Summary}
Results from this preliminary experiment strongly support our hypothesis that  structure of RNN could have impact on the quality of  explanation.  In particular,  as presented in \addfigure{\ref{fig:class_1_comparison}} and \addfigure{\ref{fig:exp1_dist_plot}}, quality of DTD and $\lrpp$ explanation are significantly influenced by the architecture. In contrast, we do not see such notable effect from SA and GB method.  In the following, we are going to discuss similar experiments that are designed in a more constructive way. This construction enables us to methodologically evaluate the impact.


\section{Experiment 2 : Majority Sample Sequence Classification} \label{sec:exp2}
   
 \begin{figure}[!htb]
\centering
\includegraphics[width=0.8\textwidth]{sketch/artificial_problem_3digits}
\caption{Majority Sample Sequence Classification(MAJ) problem.} 
\label{fig:artificial_problem_3digits}
\end{figure}

\subsection{Problem Formulation} \label{sec:exp2_prob_formulate}
When neural networks are trained, one can apply explantation techniques to the models to get explanations of the outputs.  The explanation  of sample $\x$ indicates important features in $x$ that the trained network rely on to perform the objective task,  such as classification.  Therefore, one needs to know the ground truth where these latent features are in $\x$ in order to methodologically evaluate the explainability of the model.  However, this knowledge is not trivially available because it is an incident from the training process operating in high-dimensional space that we seek to understand in the first place.

To alleviate this challenge, we propose another artificial classification problem where RNN are trained to classify  the majority group in a sequence $\x$, $(\x_t )_{t=1}^{T}$. Consider MNIST. $\x$ is constructed as follows: for each original sample $\widetilde{\x} \in \mathbb{R}^{28,28}$, we randomly selected 2 additional samples : one from the same class of $\widetilde{\x}$ and the other one from a different class. Then, these 3 samples are concatenated in random order yielding a sample $\x \in \mathbb{R}^{28,84}$.  \addfigure{\ref{fig:artificial_problem_3digits}} illustrates the construction and the objective classification. Given $\x = \{ 8, 7, 8\}$, the classification result is ``8".  We call this problem as MNIST-MAJ when $\x$ are constructed from MNIST samples and the same to FashionMNIST-MAJ.

Because we already know blocks of digit/item that belong to the majority group from the construction,  we can then use this information to  quantitively evaluate  explanation quality  of each RNN.


As discussed in the previous experiment that only some DTD and $\lrpp$ explanations from the Deep architecture on FashionMNIST were sound. This seems to suggest that the architecture might not have enough capability to extract proper representations from FashionMNIST samples, causing the incorrect propagation issue.

Hence, apart of Shallow and Deep architecture, we are going to  introduce another two architectures, namely DeepV2 and ConvDeep. The DeepV2 architecture has one more layer after the first fully-connected layer than the Deep cell. On the other hand, the ConvDeep architecture instead replaces the first layer with  a sequence of convolutional and pooling operation. \addfigure{\ref{fig:deep_conv_arch}} shows details of these new architectures.
% \todo{input padding }


%\subsection{Setting}
%Two variations of \rnncell{Deep} cell are also experimented, namely \rnncell{DeepV2} and \rnncell{ConvDeep}, shown on \addfigure{\ref{fig:deep_conv_arch}}. The former has one additional layer \circled{1"} with dropout regularization  between \circled{1'}. On the other hand, the latter replaces fully connected layers between \circled{1} and \circled{3} with 2 convolutional and max pooling layers, \Big[\circled{C1}, \circled{P1}\Big] and \Big[\circled{C2},\circled{P2}\Big].

\begin{figure}[!htb]
\centering

\subfloat[DeepV2\label{fig:deep_4l_network}]{%
       \includegraphics[width=0.48\textwidth]{sketch/deep_v2_arch}
     }
     \hfill
     \subfloat[ConvDeep\label{fig:convdeep_4l_network}]{%
       \includegraphics[width=0.48\textwidth]{sketch/convdeep_arch}
     }

\patcaption{DeepV2 and ConvDeep architecture}{with number of neurons at each layer depicted.}
\label{fig:deep_conv_arch}
\end{figure}

Lastly, despite the fact that  our implementation is readily to apply on different sequence lengths,  we conducted the experiment with only sequence length $T=12$, more precisely $(\x_t \in \mathbb{R}^{28,7})_{t=1}^{12}$. This is mainly due to computational resources and time constraint we had. Consequently, we are going to write only the name of architecture without explicitly stating the sequence length as we previously proposed.


\subsection{Evaluation Methodology}
\label{sec:evaluation_med}
From the problem construction, we know that relevance quantities should  be primarily assigned to blocks that belong to the majority group. This construction enables us to both directly visually examine quality of explanations as well as performing quantitative evaluations.  In particular, for qualitative inspections, we constructed training and testing data based on the original training and testing split that \cite{LeCunMNISThandwrittendigit2010} and \cite{XiaoFashionMNISTNovelImage2017} proposed and trained with setting described in Section \ref{sec:setup}. 

\subsubsection{Quantitative Evaluation}
A straightforward way to quantitatively evaluate results is to calculate the percentage of relevance correctly distributed the blocks belonging to the majority group digit/item. However, this measurement has a shortcoming where architectures can achieve high score if they distribute relevance to only one of the correct blocks. Hence, we then propose to use \textit{cosine similarity} instead. The cosine similarity is computed from  a  binary  vector $\patvector{m} \in \mathbb{R}^3$  whose values represent correctness of the blocks and a vector $\patvector{\upsilon} \in \mathbb{R}^3$ containing percentage of  relevance distributed to the blocks. 

\begin{align}
\cos (\patvector{m}, \patvector{\upsilon}) = \frac{ \patvector{m} \cdot \patvector{\upsilon}}{ || \patvector{m}  ||_2 ||\patvector{\upsilon}   ||_2}	
\end{align}

As illustrated in \addfigure{\ref{fig:quantitative_evaluation}}, the percentage of correctly distributed relevance can be significantly high although the relevance heatmap does not show any highlight at the left most block of ``0". Therefore, using cosine similarity is more reasonable. In fact, the propagation needs to be equally balanced between the two blocks in order to achieve the highest score, ``1". For $\lrpp$ heatmaps, we ignore negative relevance and set it to zero before computing cosine similarity.

\begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{sketch/quantitative_evaluation}
\patcaption{Comparison between percentage of correctly distributed relevance and cosine similarity.}{} 
\label{fig:quantitative_evaluation}
\end{figure}

To reduce variations possibly introduced by, for example variable initialization, we conducted quantitative evaluations through $k$-fold cross-validation process. We combined  training and testing set together and split the data into $k$ folds. Each fold is used as the testing set once. For each cross-validation iteration, we average the cosine similarity across testing samples. The final result is then averaged overall the folds' statistics.  To keep the same proportion of training and testing data, we chose $k=7$. 




%\todo{
%\subsubsection{Statistical Evaluation}
%hypo : check whether we need it: 
%It is also possible that some architectures might perform similarly and the difference is not visually observed. For such scenarios, we will use one-way ANOVA on statistics of cosine similarity and pairwise Tukey Honest Significant Difference (HSD) as a post-hoc test to verify whether there are statistically significant results. We use significance level at $0.05$. Dataset is considered as a confounding variable. This procedure is conducted separately for each explanation method.
%}



 %Table x show accuracy sf

\subsection{Result}

\renewcommand{\arraystretch}{1.5}
\begin{table}[!hbt]
\begin{center}
\begin{tabular}{lc|c|c|}
\cline{3-4}
& &
\multicolumn{2}{c|}{\parbox{3.5cm}{ \vskip 1mm \centering \textbf{Accuracy} \vskip 1mm}} \\ \hline
\multicolumn{1}{|l|}{\textbf{Cell architecture}} & \textbf{No. variables} & \textbf{MNIST-MAJ} & \textbf{FashionMNIST-MAJ} \\ \hline
\multicolumn{1}{|l|}{Shallow}    & 184,330          & 98.12\% & 90.00\% \\ 
\multicolumn{1}{|l|}{Deep}       & 153,578           & 98.16\% & 89.81\% \\ 
 \multicolumn{1}{|l|}{DeepV2}     & 161,386        & 98.26\% & 90.57\% \\
\multicolumn{1}{|l|}{ConvDeep}   & 151,802       & 99.22\% & 92.87\%  \\ \hline 
\end{tabular}

\end{center}
\caption{Number of trainable variables and model accuracy from architectures trained on MNIST-MAJ and FashionMNIST-MAJ with sequence length $T=12$.}
\label{tab:maj_rnn_model_acc}
\end{table}
\renewcommand{\arraystretch}{1}

 \begin{figure}[!htb]
\centering
\includegraphics[width=\textwidth]{sketch/heatmap_msc_for_thesis}
\patcaption{Relevance heatmaps from different explanation techniques applied to Shallow, Deep, DeepV2 and ConvDeep architecture trained on MNIST-MAJ and FashionMNIST-MAJ with sequence length $T=12$.}{\heatmapscaleexplain } 
\label{fig:heatmap_msc_mix_for_thesis}
\end{figure}

Table \ref{tab:maj_rnn_model_acc} shows number of trainable variables and accuracy of the trained models. These trained models have equivalent number of variables and accuracy, except that ConvDeep has slightly higher accuracy. \addfigure{\ref{fig:heatmap_msc_mix_for_thesis}} shows that deeper architectures have better relevance propagation, in other words, they are more explainable in an aspect of prediction. In particular, we can see that portion of relevant scores distributed to irrelevant region are gradually reduced from Shallow to ConvDeep architecture. This effect happens across all explanation methods. This result further agrees with the evidence discussed in Section \ref{sec:exp1}.  

Although explanation heatmaps from \rnncell{Shallow}, \rnncell{Deep}, and \rnncell{DeepV2} generally look noisy, increasing the depth of architecture seems to reduce the noise in the heatmaps as well.   On the other hand, \rnncell{ConvDeep} does not  only properly assign relevance quantities to the right time steps, it also produces sound heatmaps containing features of $\x$ that are not easily observed from explanation from the other architectures.  GB and $\lrpp$ heatmaps of Digit 1 and Sandal sample are such examples.

\clearpage

 \begin{figure}[!hbt]
\centering
\includegraphics[width=\textwidth]{sketch/rel_dist_maj_3_samples_thesis}

\patcaption{Average cosine similarity from different explanation techniques and Shallow, Deep, DeepV2 and ConvDeep architecture.}{The values are averaged from cross-validation results and the vertical lines depicted 95\% confidence interval. The baseline is the Shallow architecture depicted by the blue line. Accuracy of the models can be found at Appendix  \ref{annex:model_acc}}


\label{fig:rel_dist_maj_3_samples_thesis}
\end{figure}

\addfigure{\ref{fig:rel_dist_maj_3_samples_thesis}} presents quantitive  evaluations of the impact from the depth of architecture to the quality of explanation. As a reminder, the measurement is cosine similarity between a mark vector $\boldsymbol{m} \in \mathbb{R}^3$ and an aggregated relevance to blocks of digit/item vector $\boldsymbol{\upsilon \in \mathbb{R}^3 }$ and averaged through $7$-fold cross-validation procedure as described in Section \ref{sec:evaluation_med}. Results from the figure indicate that the depth of architecture indeed improves quality of the explanations. In particular, the averaged cosine similarity of each explanation technique systematically increases when introducing  more layers. This effect can be seen clearly from the result of FashionMNIST-MAJ. Additionally, we can also observe that the difference of the metric between the baseline, \rnncell{Shallow}, and the other  architectures changes with different proposition across methods. In particular, we see the difference from DTD and $\lrpp$ are much larger than the other methods. This implies that some explanation methods are more sensitive to architecture configuration than the others.


%\todo{hypo : pair wise statistical testing}

\subsection{Summary}
Results of this experiment quantitively confirm that architecture of RNN is indeed an important factor to the explainability of RNN models, especially in the aspect of propagating relevance to corresponding input steps.  The results also shows that this impact affects the quality of explanation in different level on different methods. More precisely, deep Taylor decomposition(DTD) and Layer-Wise Relevance Propagations(LRP) technique are more sensitive to the architecture of the explained model than sensitivity analysis(SA) and guided backprop(GB) method.

Nonetheless, we  also observed that there are some samples whose significant amount of relevance scores are distributed to irrelevant regions.  Digit ``9" sample on \addfigure{\ref{fig:heatmap_msc_mix_for_thesis}} is one of them. This issue is considerably obvious on ConvDeep.   Therefore, we are going to purpose several improvements to mitigate the issue.

